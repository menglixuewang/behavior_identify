from ultralytics import YOLO
import numpy as np
import os, cv2, time, torch, random, warnings, argparse, math
import threading
import queue
import contextlib

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
warnings.filterwarnings("ignore", category=UserWarning)

# æ·»åŠ ç¯å¢ƒå˜é‡æ§åˆ¶GUIæ˜¾ç¤ºï¼Œç”¨äºæœåŠ¡å™¨ç¯å¢ƒ
# Windowsç»„å‘˜å¯ä»¥å¿½ç•¥è¿™ä¸ªï¼Œé»˜è®¤å¯ç”¨GUIæ˜¾ç¤º
ENABLE_GUI = os.environ.get('ENABLE_GUI', 'true').lower() in ('true', '1', 'yes')

from pytorchvideo.transforms.functional import (
    uniform_temporal_subsample,
    short_side_scale_with_boxes,
    clip_boxes_to_image, )
from torchvision.transforms._functional_video import normalize
from pytorchvideo.data.ava import AvaLabeledVideoFramePaths
from pytorchvideo.models.hub import slowfast_r50_detection
from deep_sort.deep_sort import DeepSort


class MyVideoCapture:

    def __init__(self, source):
        # åŒºåˆ†æ‘„åƒå¤´å’Œè§†é¢‘æ–‡ä»¶
        if isinstance(source, int):
            # æ·»åŠ CAP_DSHOWåç«¯ä»¥è§£å†³Windowsæ‘„åƒå¤´è®¿é—®é—®é¢˜
            self.cap = cv2.VideoCapture(source)
            err_msg = f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´: {source}ï¼Œè¯·æ£€æŸ¥æ‘„åƒå¤´æ˜¯å¦è¢«å ç”¨æˆ–æƒé™æ˜¯å¦å…è®¸"
        else:
            # æ‰“å¼€è§†é¢‘æ–‡ä»¶
            self.cap = cv2.VideoCapture(source)
            err_msg = f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {source}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®æˆ–æ–‡ä»¶æ˜¯å¦æŸå"

        if not self.cap.isOpened():
            raise RuntimeError(err_msg)
        # ç§»é™¤æ˜¾å¼åˆ†è¾¨ç‡è®¾ç½®ï¼Œä½¿ç”¨æ‘„åƒå¤´é»˜è®¤åˆ†è¾¨ç‡
        # self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        # self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        # éªŒè¯å®é™…åˆ†è¾¨ç‡
        actual_width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)
        actual_height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
        print(f"è§†é¢‘æºåˆ†è¾¨ç‡: {actual_width}x{actual_height}")
        self.idx = -1
        self.end = False
        self.stack = []

    def read(self):
        self.idx += 1
        ret, img = self.cap.read()
        if not ret:
            print(f"è­¦å‘Š: æ— æ³•è¯»å–è§†é¢‘å¸§ {self.idx}ï¼Œå¯èƒ½æ˜¯æ‘„åƒå¤´æ–­å¼€è¿æ¥")
            self.end = True
            return ret, None
        if img is None or img.size == 0:
            print(f"è­¦å‘Š: è¯»å–åˆ°ç©ºå›¾åƒ {self.idx}")
            self.stack.append(np.zeros((480, 640, 3), dtype=np.uint8))  # è¿”å›é»‘è‰²å ä½å›¾åƒ
            return ret, img
        # æ˜¾ç¤ºåŸå§‹æ‘„åƒå¤´å¸§ç”¨äºè°ƒè¯• (ä»…åœ¨GUIæ¨¡å¼ä¸‹)
        if ENABLE_GUI:
            try:
                cv2.imshow("åŸå§‹æ‘„åƒå¤´ç”»é¢", img)
                cv2.waitKey(1)
            except cv2.error as e:
                print(f"GUIæ˜¾ç¤ºå¤±è´¥ (è¿™åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­æ˜¯æ­£å¸¸çš„): {e}")
        print(f"åŸå§‹å›¾åƒå°ºå¯¸: {img.shape}, æ•°æ®ç±»å‹: {img.dtype}, æ•°æ®èŒƒå›´: [{img.min()}, {img.max()}]")
        self.stack.append(img)
        return ret, img

    def to_tensor(self, img):
        img = torch.from_numpy(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        return img.unsqueeze(0)

    def get_video_clip(self):
        assert len(self.stack) > 0, "clip length must large than 0 !"
        self.stack = [self.to_tensor(img) for img in self.stack]
        clip = torch.cat(self.stack).permute(-1, 0, 1, 2)
        del self.stack
        self.stack = []
        return clip

    def release(self):
        """é‡Šæ”¾æ‘„åƒå¤´èµ„æºï¼Œç¡®ä¿å®Œå…¨å…³é—­"""
        try:
            if hasattr(self, 'cap') and self.cap is not None:
                print("ğŸ¥ MyVideoCapture: æ­£åœ¨é‡Šæ”¾æ‘„åƒå¤´...")
                self.cap.release()
                print("ğŸ¥ MyVideoCapture: æ‘„åƒå¤´å·²é‡Šæ”¾")

                # å¼ºåˆ¶ç­‰å¾…ç¡®ä¿èµ„æºå®Œå…¨é‡Šæ”¾
                import time
                time.sleep(0.1)

                # è®¾ç½®ä¸ºNoneï¼Œé¿å…é‡å¤é‡Šæ”¾
                self.cap = None
                print("ğŸ¥ MyVideoCapture: æ‘„åƒå¤´å¯¹è±¡å·²æ¸…ç©º")
            else:
                print("ğŸ¥ MyVideoCapture: æ‘„åƒå¤´å¯¹è±¡ä¸å­˜åœ¨æˆ–å·²é‡Šæ”¾")
        except Exception as e:
            print(f"ğŸ¥ MyVideoCapture: é‡Šæ”¾æ‘„åƒå¤´æ—¶å‡ºé”™: {e}")
        finally:
            # ç¡®ä¿å¯¹è±¡è¢«æ ‡è®°ä¸ºå·²é‡Šæ”¾
            self.end = True


def tensor_to_numpy(tensor):
    img = tensor.cpu().numpy().transpose((1, 2, 0))
    return img


def ava_inference_transform(
        clip,
        boxes,
        num_frames=32,  # if using slowfast_r50_detection, change this to 32, 4 for slow
        crop_size=640,
        data_mean=[0.45, 0.45, 0.45],
        data_std=[0.225, 0.225, 0.225],
        slow_fast_alpha=4,  # if using slowfast_r50_detection, change this to 4, None for slow
):
    boxes = np.array(boxes)
    roi_boxes = boxes.copy()
    clip = uniform_temporal_subsample(clip, num_frames)
    clip = clip.float()
    clip = clip / 255.0
    height, width = clip.shape[2], clip.shape[3]
    boxes = clip_boxes_to_image(boxes, height, width)
    clip, boxes = short_side_scale_with_boxes(clip, size=crop_size, boxes=boxes, )
    clip = normalize(clip,
                     np.array(data_mean, dtype=np.float32),
                     np.array(data_std, dtype=np.float32), )
    boxes = clip_boxes_to_image(boxes, clip.shape[2], clip.shape[3])
    if slow_fast_alpha is not None:
        fast_pathway = clip
        slow_pathway = torch.index_select(clip, 1,
                                          torch.linspace(0, clip.shape[1] - 1, clip.shape[1] // slow_fast_alpha).long())
        clip = [slow_pathway, fast_pathway]

    return clip, torch.from_numpy(boxes), roi_boxes


def plot_one_box(x, img, color=[100, 100, 100], text_info="None",
                 velocity=None, thickness=1, fontsize=0.5, fontthickness=1):
    # Plots one bounding box on image img
    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))
    cv2.rectangle(img, c1, c2, color, thickness, lineType=cv2.LINE_AA)
    t_size = cv2.getTextSize(text_info, cv2.FONT_HERSHEY_TRIPLEX, fontsize, fontthickness + 2)[0]
    cv2.rectangle(img, c1, (c1[0] + int(t_size[0]), c1[1] + int(t_size[1] * 1.45)), color, -1)
    cv2.putText(img, text_info, (c1[0], c1[1] + t_size[1] + 2),
                cv2.FONT_HERSHEY_TRIPLEX, fontsize, [255, 255, 255], fontthickness)
    return img


def deepsort_update(Tracker, pred, xywh, np_img):
    outputs = Tracker.update(xywh, pred[:, 4:5], pred[:, 5].tolist(), cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB))
    # ä¿®æ­£åçš„è·Ÿè¸ªä¿¡æ¯æ‰“å°
    print(f"\nè·Ÿè¸ªç»“æœ - ç›®æ ‡æ•°: {len(outputs)}")
    for i, output in enumerate(outputs):
        # æ£€æŸ¥è¾“å‡ºé•¿åº¦å¹¶ç›¸åº”å¤„ç†
        if len(output) >= 7:  # è‡³å°‘æœ‰ä½ç½®ã€ç±»åˆ«ã€IDå’Œç½®ä¿¡åº¦
            track_info = f"ç›®æ ‡ {i + 1}: ID:{int(output[5])} | ç±»åˆ«:{int(output[4])} | ç½®ä¿¡åº¦:{output[6]:.2f} | "
            track_info += f"ä½ç½®:[{output[0]:.0f},{output[1]:.0f},{output[2]:.0f},{output[3]:.0f}]"
            # æ£€æŸ¥æ˜¯å¦æœ‰é€Ÿåº¦ä¿¡æ¯
            if len(output) >= 8:
                track_info += f" | é€Ÿåº¦(vx,vy):[{output[6]:.2f},{output[7]:.2f}]"
            print(track_info)
    return outputs


def save_yolopreds_tovideo(yolo_preds, id_to_ava_labels, color_map, output_video, vis=False):
    for i, (im, pred) in enumerate(zip(yolo_preds.ims, yolo_preds.pred)):
        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
        if pred.shape[0]:
            for j, (*box, cls, trackid, vx, vy) in enumerate(pred):
                if int(cls) != 0:
                    ava_label = ''
                elif trackid in id_to_ava_labels.keys():
                    ava_label = id_to_ava_labels[trackid].split(' ')[0]
                else:
                    ava_label = 'Unknow'
                text = '{} {} {}'.format(int(trackid), yolo_preds.names[int(cls)], ava_label)
                color = color_map[int(cls)]
                im = plot_one_box(box, im, color, text)
        im = im.astype(np.uint8)
        # å¼ºåˆ¶å›¾åƒæ•°æ®ç±»å‹ä¸ºuint8å¹¶è£å‰ªèŒƒå›´ï¼Œè§£å†³é»‘å±é—®é¢˜
        im = np.clip(im, 0, 255).astype(np.uint8)
        # æ·»åŠ å›¾åƒè°ƒè¯•ä¿¡æ¯
        print(f"æ˜¾ç¤ºå›¾åƒå°ºå¯¸: {im.shape}, æ•°æ®ç±»å‹: {im.dtype}, æ•°æ®èŒƒå›´: [{im.min()}, {im.max()}]")
        if im.size == 0:
            print("è­¦å‘Š: ç©ºå›¾åƒï¼Œè·³è¿‡æ˜¾ç¤º")
            return
        if output_video is not None:
            output_video.write(im)
        if vis:
            # æ¢å¤æ­£ç¡®çš„é¢œè‰²ç©ºé—´è½¬æ¢ (RGBè½¬BGR)
            im_bgr = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)
            # æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            print(
                f"æ˜¾ç¤ºå‰å›¾åƒå°ºå¯¸: {im_bgr.shape}, æ•°æ®ç±»å‹: {im_bgr.dtype}, æ•°æ®èŒƒå›´: [{im_bgr.min()}, {im_bgr.max()}]")
            if ENABLE_GUI:
                try:
                    cv2.imshow("å®æ—¶æ£€æµ‹", im_bgr)
                    key = cv2.waitKey(1)
                    if key & 0xFF == ord('q'):
                        cv2.destroyAllWindows()
                        exit()
                except cv2.error as e:
                    print(f"GUIæ˜¾ç¤ºå¤±è´¥ (è¿™åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­æ˜¯æ­£å¸¸çš„): {e}")


def main(config):
    device = config.device
    imsize = config.imsize

    # åˆ¤æ–­æ˜¯å¦ä¸ºå®æ—¶æ‘„åƒå¤´æ¨¡å¼
    is_camera = isinstance(config.input, int)

    model = YOLO("yolov8n.pt")  # yolov8l, s, m, x

    # ä¿®æ”¹ SlowFast æ¨¡å‹åŠ è½½æ–¹å¼
    SLOWFAST_WEIGHTS_PATH = "SLOWFAST_8x8_R50_DETECTION.pyth"
    if not os.path.exists(SLOWFAST_WEIGHTS_PATH):
        raise RuntimeError(f"SlowFast æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {SLOWFAST_WEIGHTS_PATH}")
    
    # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
    video_model = slowfast_r50_detection(False)  # Falseè¡¨ç¤ºä¸è‡ªåŠ¨ä¸‹è½½æƒé‡
    checkpoint = torch.load(SLOWFAST_WEIGHTS_PATH)
    video_model.load_state_dict(checkpoint['model_state'])
    video_model = video_model.eval().to(device)

    deepsort_tracker = DeepSort("ckpt.t7")
    ava_labelnames, _ = AvaLabeledVideoFramePaths.read_label_map("temp.pbtxt")

    coco_color_map = [[random.randint(0, 255) for _ in range(3)] for _ in range(80)]

    vide_save_path = config.output
    if not vide_save_path and not is_camera:
        # å¦‚æœæ˜¯è§†é¢‘æ–‡ä»¶ä½†æ²¡æœ‰æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
        input_name = os.path.splitext(os.path.basename(config.input))[0]
        vide_save_path = f"{input_name}_output.mp4"
        print(f"æœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œå°†ä¿å­˜åˆ°: {vide_save_path}")

    outputvideo = None
    if vide_save_path:
        video = cv2.VideoCapture(config.input)
        width, height = int(video.get(3)), int(video.get(4))
        video.release()
        outputvideo = cv2.VideoWriter(vide_save_path, cv2.VideoWriter_fourcc(*"mp4v"), 25, (width, height))
        print(f"è§†é¢‘å°†ä¿å­˜åˆ°: {os.path.abspath(vide_save_path)}")
    else:
        # Get camera resolution
        cap = cv2.VideoCapture(config.input)
        width = int(cap.get(3))
        height = int(cap.get(4))
        cap.release()

    print("å¼€å§‹å¤„ç†...")
    cap = MyVideoCapture(config.input)
    id_to_ava_labels = {}
    a = time.time()

    # æ–°å¢ï¼šclip é˜Ÿåˆ—å’ŒåŠ¨ä½œè¯†åˆ«çº¿ç¨‹
    clip_queue = queue.Queue()
    result_queue = queue.Queue()

    def slowfast_worker():
        # ä»…å½“ä½¿ç”¨GPUæ—¶æ‰åˆ›å»ºç‹¬ç«‹çš„CUDAæµ
        stream = torch.cuda.Stream() if 'cuda' in str(device) else None
        
        # ä½¿ç”¨ contextlib.nullcontext() æ¥ä¼˜é›…åœ°å¤„ç† CPU æƒ…å†µ
        context_manager = torch.cuda.stream(stream) if stream else contextlib.nullcontext()

        while True:
            item = clip_queue.get()
            if item is None:
                break
            idx, clip, pred_result = item
            
            # æ‰€æœ‰åœ¨è¿™ä¸ª 'with' å—å†…çš„CUDAæ“ä½œéƒ½å°†åœ¨æˆ‘ä»¬åˆ›å»ºçš„ç‹¬ç«‹æµä¸Šæ‰§è¡Œ
            with context_manager:
                if pred_result.pred[0].shape[0]:
                    inputs, inp_boxes, _ = ava_inference_transform(clip, pred_result.pred[0][:, 0:4], crop_size=imsize)
                    inp_boxes = torch.cat([torch.zeros(inp_boxes.shape[0], 1), inp_boxes], dim=1)
                    if isinstance(inputs, list):
                        # non_blocking=True å…è®¸ä¸»æœº(CPU)ä»£ç ç»§ç»­æ‰§è¡Œï¼Œè€Œä¸ä¼šç­‰å¾…æ•°æ®å¤åˆ¶å®Œæˆ
                        inputs = [inp.unsqueeze(0).to(device, non_blocking=True) for inp in inputs]
                    else:
                        inputs = inputs.unsqueeze(0).to(device, non_blocking=True)
                    
                    inp_boxes_gpu = inp_boxes.to(device, non_blocking=True)

                    with torch.no_grad():
                        # SlowFastæ¨¡å‹æ¨ç†è¢«æäº¤åˆ°ç‹¬ç«‹çš„æµ
                        slowfaster_preds = video_model(inputs, inp_boxes_gpu)
                    
                    # ä¿®å¤æ•°æ®ç±»å‹è½¬æ¢é—®é¢˜ - ç¡®ä¿æ­£ç¡®çš„æ•°æ®ç±»å‹è½¬æ¢
                    slowfaster_preds_cpu = slowfaster_preds.cpu().float()  # ç¡®ä¿ä¸ºfloatç±»å‹
                    pred_labels = torch.argmax(slowfaster_preds_cpu, dim=1).numpy().astype(np.int32)
                    track_ids = pred_result.pred[0][:, 5].astype(np.int32)

                    result_queue.put((idx, track_ids.tolist(), pred_labels.tolist()))
            clip_queue.task_done()

    threading.Thread(target=slowfast_worker, daemon=True).start()

    frame_count = 0
    total_frames = int(cap.cap.get(cv2.CAP_PROP_FRAME_COUNT)) if not is_camera else 0

    while not cap.end:
        ret, img = cap.read()
        if not ret:
            continue

        frame_count += 1
        if not is_camera and frame_count % 25 == 0:
            # æ¯25å¸§æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ï¼ˆä»…åœ¨å¤„ç†è§†é¢‘æ–‡ä»¶æ—¶ï¼‰
            progress = (frame_count / total_frames) * 100 if total_frames > 0 else 0
            print(f"\rå¤„ç†è¿›åº¦: {progress:.1f}% ({frame_count}/{total_frames})", end="", flush=True)

        results = model.predict(source=img, imgsz=imsize, device=device, verbose=False)
        boxes = results[0].boxes  # YOLOv8 Results object

        # ä»…åœ¨è°ƒè¯•æ¨¡å¼æˆ–ä½¿ç”¨æ‘„åƒå¤´æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯
        if is_camera:
            print(f"\nå¸§ {cap.idx} - æ£€æµ‹ç»“æœ:")
            print(f"æ£€æµ‹åˆ° {len(boxes)} ä¸ªç›®æ ‡:")
            cls_counts = {}
            for i, box in enumerate(boxes):
                cls_name = model.names[int(box.cls)]
                cls_counts[cls_name] = cls_counts.get(cls_name, 0) + 1
                print(f"ç›®æ ‡ {i + 1}: ç±»åˆ«:{cls_name} | ç½®ä¿¡åº¦:{box.conf.item():.2f} | "
                      f"ä½ç½®:[{box.xyxy.cpu().numpy()[0][0]:.0f},{box.xyxy.cpu().numpy()[0][1]:.0f},"
                      f"{box.xyxy.cpu().numpy()[0][2]:.0f},{box.xyxy.cpu().numpy()[0][3]:.0f}]")
            print("ç±»åˆ«ç»Ÿè®¡:", ", ".join([f"{k}:{v}" for k, v in cls_counts.items()]))

        pred_xyxy = boxes.xyxy.cpu().numpy()
        pred_conf = boxes.conf.cpu().numpy().reshape(-1, 1)
        pred_cls = boxes.cls.cpu().numpy().reshape(-1, 1)

        pred = np.hstack((pred_xyxy, pred_conf, pred_cls))
        xywh = np.hstack(((pred[:, 0:2] + pred[:, 2:4]) / 2, pred[:, 2:4] - pred[:, 0:2]))
        temp = deepsort_update(deepsort_tracker, pred, xywh, img)
        temp = temp if len(temp) else np.ones((0, 8)).astype(np.float32)

        pred_result = type("YoloPred", (), {})()
        pred_result.ims = [img]
        pred_result.pred = [temp.astype(np.float32)]
        pred_result.names = model.names

        if len(cap.stack) == 25:
            if is_camera:
                print(f"processing {cap.idx // 25}th second clips (å¼‚æ­¥)")
            clip = cap.get_video_clip()
            clip_queue.put((cap.idx, clip, pred_result))

        while not result_queue.empty():
            idx, tids, avalabels = result_queue.get()
            for tid, avalabel in zip(tids, avalabels):
                id_to_ava_labels[tid] = ava_labelnames[avalabel + 1]
            if is_camera:
                print(f"åŠ¨ä½œè¯†åˆ«ç»“æœ: æ›´æ–°äº† {len(avalabels)} ä¸ªç›®æ ‡çš„åŠ¨ä½œæ ‡ç­¾ (å¸§ {idx})")
            result_queue.task_done()

        # ä»…åœ¨ä½¿ç”¨æ‘„åƒå¤´æ—¶æ˜¾ç¤ºå®æ—¶ç”»é¢
        show_frame = is_camera and config.show
        save_yolopreds_tovideo(pred_result, id_to_ava_labels, coco_color_map, outputvideo, show_frame)

    if not is_camera:
        print("\n")  # ä¸ºè¿›åº¦æ¡æ·»åŠ æ¢è¡Œ

    print("\nå¤„ç†å®Œæˆ!")
    process_time = time.time() - a
    print("æ€»è€—æ—¶: {:.3f} ç§’, è§†é¢‘é•¿åº¦: {:.1f} ç§’, å¹³å‡å¸§ç‡: {:.1f} FPS".format(
        process_time, cap.idx / 25, frame_count / process_time))
    
    cap.release()
    if outputvideo is not None:
        outputvideo.release()
        print('è§†é¢‘å·²ä¿å­˜åˆ°:', os.path.abspath(vide_save_path))
    cv2.destroyAllWindows()
    clip_queue.put(None)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, default='0')
    parser.add_argument('--output', type=str, default='')
    parser.add_argument('--imsize', type=int, default=640)
    parser.add_argument('--conf', type=float, default=0.4)
    parser.add_argument('--iou', type=float, default=0.4)
    parser.add_argument('--device', default='cpu')
    parser.add_argument('--classes', nargs='+', type=int)
    parser.add_argument('--show', action='store_false', default=True, help='Show real-time video')
    config = parser.parse_args()

    if config.input.isdigit():
        print("using local camera.")
        config.input = int(config.input)

    print(config)
    main(config)
