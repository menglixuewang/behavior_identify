"""
æ£€æµ‹æœåŠ¡æ¨¡å— - å°è£…YOLOv8+SlowFastç®—æ³•
"""
import os
import sys
import cv2
import time
import json
import threading
import queue
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any

# æ·»åŠ ç®—æ³•æ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
yolo_slowfast_path = os.path.join(project_root, 'yolo_slowfast-master')
sys.path.append(yolo_slowfast_path)

# å¯¼å…¥ç°æœ‰ç®—æ³•æ¨¡å—
try:
    from yolo_slowfast import *
    import torch
    import numpy as np
    from ultralytics import YOLO
    from pytorchvideo.data.ava import AvaLabeledVideoFramePaths
    from pytorchvideo.models.hub import slowfast_r50_detection
    from deep_sort.deep_sort import DeepSort
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥ç®—æ³•æ¨¡å—: {e}")


class BehaviorDetectionService:
    """è¡Œä¸ºæ£€æµ‹æœåŠ¡ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ£€æµ‹æœåŠ¡
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«è®¾å¤‡ã€è¾“å…¥å°ºå¯¸ã€ç½®ä¿¡åº¦é˜ˆå€¼ç­‰å‚æ•°
        """
        # è®¾å¤‡é…ç½® - ä¿®å¤GPUæ£€æµ‹
        if config.get('device', 'cpu').lower() == 'cuda':
            if torch.cuda.is_available():
                self.device = 'cuda'
                print(f"âœ“ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            else:
                self.device = 'cpu'
                print("âš  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
        else:
            self.device = 'cpu'
            print("âœ“ ä½¿ç”¨CPU")
        
        # åŠ è½½COCOç±»åˆ«åç§°
        coco_names_path = os.path.join(yolo_slowfast_path, 'selfutils', 'coco_names.txt')
        self.coco_names = []
        if os.path.exists(coco_names_path):
            with open(coco_names_path, 'r') as f:
                self.coco_names = [line.strip() for line in f.readlines()]
            print(f"âœ“ åŠ è½½COCOç±»åˆ«åç§°: {len(self.coco_names)}ä¸ªç±»åˆ«")
        else:
            print("âš  COCOç±»åˆ«åç§°æ–‡ä»¶ä¸å­˜åœ¨")
            # ä½¿ç”¨é»˜è®¤ç±»åˆ«
            self.coco_names = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane']
        
        self.input_size = config.get('input_size', 640)
        self.confidence_threshold = config.get('confidence_threshold', 0.5)

        # æ‰“å°æ¥æ”¶åˆ°çš„é…ç½®ä¿¡æ¯
        print(f"ğŸ”§ æ£€æµ‹æœåŠ¡é…ç½®:")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - è¾“å…¥å°ºå¯¸: {self.input_size}")
        print(f"   - ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")

        # åˆå§‹åŒ–æ ‡å¿—
        self.models_initialized = False
        self.task_lock = threading.Lock()
        self.stopped_tasks = set()
        self.current_tasks = {}  # å½“å‰è¿è¡Œçš„ä»»åŠ¡ï¼Œä¿æŒå…¼å®¹æ€§

        # æ¨¡å‹ç›¸å…³è·¯å¾„
        self.yolo_model_path = 'yolov8n.pt'
        self.slowfast_weights_path = 'SLOWFAST_8x8_R50_DETECTION.pyth'
        self.deepsort_weights_path = 'ckpt.t7'
        self.ava_labels_path = 'temp.pbtxt'

        # æ¨¡å‹å¯¹è±¡
        self.yolo_model = None
        self.video_model = None
        self.deepsort_tracker = None
        self.ava_labelnames = None

        # æŠ¥è­¦é…ç½® - é»˜è®¤åªå¯¹æœ€é‡è¦çš„ä¸‰ç§å¼‚å¸¸è¡Œä¸ºæŠ¥è­¦
        self.alert_behaviors = config.get('alert_behaviors', ['fall down', 'fight', 'enter'])
        self.output_format = config.get('output_format', 'both')
        self.save_results = config.get('save_results', True)

        # åˆ›å»ºå‰ç«¯é€‰é¡¹ä¸AVAæ ‡ç­¾çš„æ˜ å°„å…³ç³»
        self.behavior_mapping = {
            'fall down': ['fall down'],
            'fight': ['fight/hit (a person)', 'martial art', 'kick (a person)', 'grab (a person)'],
            'enter': ['enter'],
            'exit': ['exit'],
            'run': ['run'],
            'sit': ['sit'],  # æ³¨æ„ï¼šAVAä¸­æœ‰ä¸¤ä¸ªsitæ ‡ç­¾(id:11å’Œid:49)
            'stand': ['stand'],  # æ³¨æ„ï¼šAVAä¸­æœ‰ä¸¤ä¸ªstandæ ‡ç­¾(id:12å’Œid:80)
            'walk': ['walk']
        }

        print(f"   - æŠ¥è­¦è¡Œä¸º: {self.alert_behaviors}")
        print(f"   - è¾“å‡ºæ ¼å¼: {self.output_format}")
        print(f"   - ä¿å­˜ç»“æœ: {self.save_results}")
        print(f"   - è¡Œä¸ºæ˜ å°„: {self.behavior_mapping}")
        
    def initialize_models(self) -> bool:
        """
        åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            print("æ­£åœ¨åˆå§‹åŒ–ç®—æ³•æ¨¡å‹...")
            
            # åˆ‡æ¢åˆ°ç®—æ³•ç›®å½•
            original_cwd = os.getcwd()
            os.chdir(yolo_slowfast_path)
            
            # åˆå§‹åŒ–YOLOæ¨¡å‹
            self.yolo_model = YOLO(self.yolo_model_path)
            print(f"âœ“ YOLOæ¨¡å‹å·²åŠ è½½: {self.yolo_model_path}")
            
            # åˆå§‹åŒ–SlowFastæ¨¡å‹
            if os.path.exists(self.slowfast_weights_path):
                self.video_model = slowfast_r50_detection(False)
                checkpoint = torch.load(self.slowfast_weights_path, map_location=self.device)
                self.video_model.load_state_dict(checkpoint['model_state'])
                self.video_model = self.video_model.eval().to(self.device)
                print(f"âœ“ SlowFastæ¨¡å‹å·²åŠ è½½: {self.slowfast_weights_path}")
            else:
                print(f"âš  SlowFastæƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {self.slowfast_weights_path}")
                self.video_model = slowfast_r50_detection(True).eval().to(self.device)
            
            # åˆå§‹åŒ–DeepSortè·Ÿè¸ªå™¨
            if os.path.exists(self.deepsort_weights_path):
                self.deepsort_tracker = DeepSort(self.deepsort_weights_path)
                print(f"âœ“ DeepSortè·Ÿè¸ªå™¨å·²åŠ è½½: {self.deepsort_weights_path}")
            else:
                # å¦‚æœç»å¯¹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ç›¸å¯¹è·¯å¾„
                relative_path = "deep_sort/deep_sort/deep/checkpoint/ckpt.t7"
                if os.path.exists(relative_path):
                    self.deepsort_tracker = DeepSort(relative_path)
                    print(f"âœ“ DeepSortè·Ÿè¸ªå™¨å·²åŠ è½½: {relative_path}")
                else:
                    print(f"âš  DeepSortæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.deepsort_weights_path}")
                    print(f"âš  ç›¸å¯¹è·¯å¾„ä¹Ÿä¸å­˜åœ¨: {relative_path}")
                    return False
            
            # åŠ è½½AVAæ ‡ç­¾
            if os.path.exists(self.ava_labels_path):
                self.ava_labelnames, _ = AvaLabeledVideoFramePaths.read_label_map(self.ava_labels_path)
                print(f"âœ“ AVAæ ‡ç­¾å·²åŠ è½½: {self.ava_labels_path}")
            else:
                print(f"âš  AVAæ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {self.ava_labels_path}")
                return False
            
            # åˆå§‹åŒ–é¢œè‰²æ˜ å°„
            self.color_map = [[random.randint(0, 255) for _ in range(3)] for _ in range(80)]
            
            # æ¢å¤åŸå§‹ç›®å½•
            os.chdir(original_cwd)
            
            self.models_initialized = True
            print("âœ“ æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # æ¢å¤åŸå§‹ç›®å½•
            try:
                os.chdir(original_cwd)
            except:
                pass
            return False
    
    def detect_video(self, video_path: str, output_path: str = None,
                    progress_callback: callable = None) -> Dict[str, Any]:
        """
        æ£€æµ‹è§†é¢‘æ–‡ä»¶

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            Dict: æ£€æµ‹ç»“æœ
        """
        print(f"ğŸ¬ å¼€å§‹è§†é¢‘æ£€æµ‹")
        print(f"ğŸ“ è¾“å…¥è§†é¢‘: {video_path}")
        print(f"ğŸ“ è¾“å‡ºè§†é¢‘: {output_path}")
        print(f"ğŸ”§ æ£€æµ‹å‚æ•°: è®¾å¤‡={self.device}, å°ºå¯¸={self.input_size}, ç½®ä¿¡åº¦={self.confidence_threshold}")
        print(f"ğŸš¨ æŠ¥è­¦è¡Œä¸º: {self.alert_behaviors}")
        print(f"ğŸ’¾ è¾“å‡ºæ ¼å¼: {self.output_format}")
        print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {self.save_results}")

        if not self.models_initialized:
            print("ğŸ”„ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            if not self.initialize_models():
                print("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
                return {'success': False, 'error': 'æ¨¡å‹åˆå§‹åŒ–å¤±è´¥'}
            print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")

        try:
            # åˆ›å»ºä»»åŠ¡ID
            task_id = f"video_{int(time.time())}"
            print(f"ğŸ†” ä»»åŠ¡ID: {task_id}")

            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆåœ¨åˆ‡æ¢ç›®å½•å‰ï¼‰
            video_path = os.path.abspath(video_path)
            if output_path:
                output_path = os.path.abspath(output_path)

            print(f"ğŸ“ ç»å¯¹è·¯å¾„ - è¾“å…¥: {video_path}")
            print(f"ğŸ“ ç»å¯¹è·¯å¾„ - è¾“å‡º: {output_path}")

            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(video_path):
                print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                return {'success': False, 'error': f'è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}'}

            # è·å–è§†é¢‘æ–‡ä»¶ä¿¡æ¯
            file_size = os.path.getsize(video_path)
            print(f"ğŸ“Š è§†é¢‘æ–‡ä»¶å¤§å°: {file_size} bytes ({file_size/1024/1024:.2f} MB)")

            # åˆ‡æ¢åˆ°ç®—æ³•ç›®å½•
            original_cwd = os.getcwd()
            print(f"ğŸ“‚ å½“å‰ç›®å½•: {original_cwd}")
            os.chdir(yolo_slowfast_path)
            print(f"ğŸ“‚ åˆ‡æ¢åˆ°ç®—æ³•ç›®å½•: {yolo_slowfast_path}")

            # å‡†å¤‡æ£€æµ‹å‚æ•°
            config = type('Config', (), {})()
            config.input = video_path
            config.output = output_path or ''
            config.imsize = self.input_size
            config.device = self.device
            config.show = False
            config.conf = self.confidence_threshold
            config.iou = 0.4
            config.classes = None

            print(f"âš™ï¸ ç®—æ³•é…ç½®:")
            print(f"   - input: {config.input}")
            print(f"   - output: {config.output}")
            print(f"   - imsize: {config.imsize}")
            print(f"   - device: {config.device}")
            print(f"   - conf: {config.conf}")
            print(f"   - iou: {config.iou}")
            
            # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
            with self.task_lock:
                self.current_tasks[task_id] = {
                    'type': 'video',
                    'status': 'running',
                    'start_time': time.time(),
                    'progress': 0.0
                }
            
            # æ‰§è¡Œæ£€æµ‹
            results = self._run_detection(config, task_id, progress_callback)
            
            # æ¢å¤åŸå§‹ç›®å½•
            os.chdir(original_cwd)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            with self.task_lock:
                if task_id in self.current_tasks:
                    self.current_tasks[task_id]['status'] = 'completed'
                    self.current_tasks[task_id]['end_time'] = time.time()
            
            return {
                'success': True,
                'task_id': task_id,
                'results': results,
                'output_path': output_path
            }
            
        except Exception as e:
            # æ¢å¤åŸå§‹ç›®å½•
            try:
                os.chdir(original_cwd)
            except:
                pass
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            with self.task_lock:
                if task_id in self.current_tasks:
                    self.current_tasks[task_id]['status'] = 'failed'
                    self.current_tasks[task_id]['error'] = str(e)
            
            return {'success': False, 'error': str(e)}
    
    def start_realtime_detection(self, source: int = 0, 
                                websocket_callback: callable = None) -> str:
        """
        å¯åŠ¨å®æ—¶æ£€æµ‹
        
        Args:
            source: æ‘„åƒå¤´ID
            websocket_callback: WebSocketå›è°ƒå‡½æ•°
            
        Returns:
            str: ä»»åŠ¡ID
        """
        if not self.models_initialized:
            if not self.initialize_models():
                raise Exception('æ¨¡å‹åˆå§‹åŒ–å¤±è´¥')
        
        task_id = f"realtime_{int(time.time())}"
        
        def realtime_worker():
            try:
                # åˆ‡æ¢åˆ°ç®—æ³•ç›®å½•
                original_cwd = os.getcwd()
                os.chdir(yolo_slowfast_path)
                
                # å‡†å¤‡æ£€æµ‹å‚æ•°
                config = type('Config', (), {})()
                config.input = source
                config.output = ''
                config.imsize = self.input_size
                config.device = self.device
                config.show = False
                config.conf = self.confidence_threshold
                config.iou = 0.4
                config.classes = None
                
                # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
                with self.task_lock:
                    self.current_tasks[task_id] = {
                        'type': 'realtime',
                        'status': 'running',
                        'start_time': time.time(),
                        'source': source
                    }
                
                # æ‰§è¡Œå®æ—¶æ£€æµ‹
                self._run_realtime_detection(config, task_id, websocket_callback)
                
                # æ¢å¤åŸå§‹ç›®å½•
                os.chdir(original_cwd)
                
            except Exception as e:
                print(f"å®æ—¶æ£€æµ‹é”™è¯¯: {e}")
                # æ¢å¤åŸå§‹ç›®å½•
                try:
                    os.chdir(original_cwd)
                except:
                    pass
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                with self.task_lock:
                    if task_id in self.current_tasks:
                        self.current_tasks[task_id]['status'] = 'failed'
                        self.current_tasks[task_id]['error'] = str(e)
        
        # å¯åŠ¨å®æ—¶æ£€æµ‹çº¿ç¨‹
        thread = threading.Thread(target=realtime_worker, daemon=True)
        thread.start()
        
        return task_id
    
    def stop_realtime_detection(self, task_id: str) -> bool:
        """
        åœæ­¢å®æ—¶æ£€æµ‹
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåœæ­¢
        """
        with self.task_lock:
            if task_id in self.current_tasks:
                self.current_tasks[task_id]['status'] = 'stopped'
                return True
        return False
    
    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            Dict: ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        with self.task_lock:
            return self.current_tasks.get(task_id, {'status': 'not_found'})
    
    def _run_detection(self, config, task_id: str, progress_callback: callable = None) -> List[Dict]:
        """
        æ‰§è¡Œæ£€æµ‹çš„æ ¸å¿ƒé€»è¾‘ï¼ˆåŸºäºç°æœ‰ç®—æ³•ï¼‰
        """
        results = []
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„mainå‡½æ•°é€»è¾‘ï¼Œä½†è¿›è¡Œäº†ä¿®æ”¹ä»¥æ”¯æŒå›è°ƒ
            cap = MyVideoCapture(config.input)
            id_to_ava_labels = {}
            
            total_frames = int(cap.cap.get(cv2.CAP_PROP_FRAME_COUNT))
            processed_frames = 0
            
            # è®¾ç½®è¾“å‡ºè§†é¢‘ - ä¿®å¤ç¼–è§£ç å™¨é—®é¢˜
            outputvideo = None
            if config.output:
                video = cv2.VideoCapture(config.input)
                width, height = int(video.get(3)), int(video.get(4))
                fps = int(video.get(cv2.CAP_PROP_FPS)) or 25
                video.release()
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(config.output), exist_ok=True)
                
                # ä½¿ç”¨æµè§ˆå™¨å…¼å®¹çš„MP4æ ¼å¼ï¼Œä¼˜å…ˆå°è¯•H.264ç¼–è§£ç å™¨
                output_mp4 = config.output.replace('.avi', '.mp4')
                
                # å°è¯•ä¸åŒçš„ç¼–è§£ç å™¨ï¼Œä¼˜å…ˆä½¿ç”¨æµè§ˆå™¨å…¼å®¹æ€§æœ€å¥½çš„
                codecs_to_try = [
                    ('avc1', 'H.264 (æœ€ä½³æµè§ˆå™¨å…¼å®¹æ€§)'),
                    ('h264', 'H.264'),
                    ('mp4v', 'MPEG-4'),
                ]
                
                outputvideo = None
                used_codec = None
                
                for codec, desc in codecs_to_try:
                    try:
                        fourcc = cv2.VideoWriter_fourcc(*codec)
                        test_writer = cv2.VideoWriter(output_mp4, fourcc, fps, (width, height))
                        
                        if test_writer.isOpened():
                            outputvideo = test_writer
                            used_codec = f"{codec} ({desc})"
                            config.output = output_mp4
                            print(f"âœ“ ä½¿ç”¨ {used_codec} ç¼–è§£ç å™¨è¾“å‡º: {output_mp4}")
                            break
                        else:
                            test_writer.release()
                    except Exception as e:
                        print(f"âš  {codec} ç¼–è§£ç å™¨å¤±è´¥: {e}")
                        continue
                
                # å¦‚æœæ‰€æœ‰MP4ç¼–è§£ç å™¨éƒ½å¤±è´¥ï¼Œå›é€€åˆ°AVI
                if not outputvideo or not outputvideo.isOpened():
                    print("âš  æ‰€æœ‰MP4ç¼–è§£ç å™¨å¤±è´¥ï¼Œå›é€€åˆ°AVIæ ¼å¼")
                    fourcc = cv2.VideoWriter_fourcc(*'XVID')
                    outputvideo = cv2.VideoWriter(config.output, fourcc, fps, (width, height))
                    if outputvideo.isOpened():
                        used_codec = "XVID (AVI)"
                        print(f"âœ“ ä½¿ç”¨ {used_codec} ç¼–è§£ç å™¨")
                    else:
                        print("âŒ æ‰€æœ‰è§†é¢‘ç¼–è§£ç å™¨éƒ½å¤±è´¥")
                        outputvideo = None
            
            while not cap.end:
                ret, img = cap.read()
                if not ret:
                    continue
                
                processed_frames += 1
                
                # YOLOæ£€æµ‹
                yolo_results = self.yolo_model.predict(
                    source=img, 
                    imgsz=config.imsize, 
                    device=config.device, 
                    verbose=False
                )
                boxes = yolo_results[0].boxes
                
                # å¤„ç†æ£€æµ‹ç»“æœ
                if len(boxes) > 0:
                    pred_xyxy = boxes.xyxy.cpu().numpy()
                    pred_conf = boxes.conf.cpu().numpy().reshape(-1, 1)
                    pred_cls = boxes.cls.cpu().numpy().reshape(-1, 1)
                    
                    pred = np.hstack((pred_xyxy, pred_conf, pred_cls))
                    xywh = np.hstack(((pred[:, 0:2] + pred[:, 2:4]) / 2, pred[:, 2:4] - pred[:, 0:2]))
                    
                    # DeepSortè·Ÿè¸ª
                    temp = deepsort_update(self.deepsort_tracker, pred, xywh, img)
                    temp = temp if len(temp) else np.ones((0, 8)).astype(np.float32)
                    
                                    # è¡Œä¸ºè¯†åˆ«ï¼ˆSlowFastï¼‰ - å…ˆè¿›è¡Œè¡Œä¸ºè¯†åˆ«å†ç»˜åˆ¶è§†é¢‘å¸§
                if len(cap.stack) == 25:
                    clip = cap.get_video_clip()
                    if temp.shape[0] > 0:
                        try:
                            # è·å–è¾¹ç•Œæ¡†ä½ç½®ï¼ˆå‰4åˆ—ï¼‰
                            boxes = temp[:, 0:4].astype(np.float32)
                            track_ids = temp[:, 5].astype(np.int32)  # è·Ÿè¸ªIDåœ¨ç¬¬5åˆ—
                            
                            inputs, inp_boxes, _ = ava_inference_transform(clip, boxes, crop_size=config.imsize)
                            
                            # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜
                            inp_boxes = inp_boxes.float()  # ç¡®ä¿ä¸ºfloatç±»å‹
                            inp_boxes = torch.cat([torch.zeros(inp_boxes.shape[0], 1), inp_boxes], dim=1)
                            
                            if isinstance(inputs, list):
                                inputs = [inp.unsqueeze(0).to(self.device) for inp in inputs]
                            else:
                                inputs = inputs.unsqueeze(0).to(self.device)
                            
                            with torch.no_grad():
                                slowfaster_preds = self.video_model(inputs, inp_boxes.to(self.device))
                            
                            # è·å–é¢„æµ‹ç»“æœ
                            pred_labels = torch.argmax(slowfaster_preds.cpu(), axis=1).numpy()
                            
                            # æ›´æ–°è¡Œä¸ºæ ‡ç­¾æ˜ å°„
                            for tid, avalabel in zip(track_ids, pred_labels):
                                if avalabel < len(self.ava_labelnames):
                                    id_to_ava_labels[int(tid)] = self.ava_labelnames[avalabel + 1]
                                    
                            print(f"âœ“ SlowFastæ£€æµ‹åˆ°{len(pred_labels)}ä¸ªè¡Œä¸ºï¼Œæ›´æ–°æ ‡ç­¾æ˜ å°„")
                            
                        except Exception as e:
                            print(f"SlowFastå¤„ç†é”™è¯¯: {e}")
                            import traceback
                            traceback.print_exc()

                # åˆ›å»ºå¯è§†åŒ–å›¾åƒ - åœ¨è¡Œä¸ºè¯†åˆ«å®Œæˆåç»˜åˆ¶
                vis_img = img.copy()
                cv2.putText(vis_img, f'Frame: {processed_frames}', 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # å­˜å‚¨æ£€æµ‹ç»“æœå¹¶ç»˜åˆ¶ï¼ˆåŒ…å«æœ€æ–°çš„è¡Œä¸ºä¿¡æ¯ï¼‰
                for detection in temp:
                    if len(detection) >= 7:
                        x1, y1, x2, y2 = int(detection[0]), int(detection[1]), int(detection[2]), int(detection[3])
                        class_id = int(detection[4])  # è¿™æ˜¯YOLOçš„ç±»åˆ«ID
                        track_id = int(detection[5])  # è¿™æ˜¯DeepSortçš„è·Ÿè¸ªID
                        confidence = float(detection[6])
                        
                        # æ˜ å°„YOLOç±»åˆ«IDåˆ°ç±»åˆ«åç§°
                        object_type = 'unknown'
                        if 0 <= class_id < len(self.coco_names):
                            object_type = self.coco_names[class_id]
                        
                        # è·å–æœ€æ–°çš„è¡Œä¸ºæ ‡ç­¾
                        behavior_type = id_to_ava_labels.get(track_id, 'walking')
                        
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        color = (0, 0, 255) if self._is_anomaly_behavior(behavior_type) else (0, 255, 0)
                        cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, 2)
                        
                        # ç»˜åˆ¶å¯¹è±¡æ ‡ç­¾ï¼ˆåŒ…å«è¡Œä¸ºä¿¡æ¯ï¼‰
                        label1 = f"ID:{track_id} {object_type}"
                        label2 = f"Action: {behavior_type}"  # ä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡ä¹±ç 
                        
                        cv2.putText(vis_img, label1, (x1, y1-25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                        cv2.putText(vis_img, label2, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                        
                        result = {
                            'frame_number': processed_frames,
                            'timestamp': processed_frames / 25.0,
                            'object_id': track_id,
                            'object_type': object_type,
                            'confidence': confidence,
                            'bbox': {
                                'x1': float(detection[0]),
                                'y1': float(detection[1]),
                                'x2': float(detection[2]),
                                'y2': float(detection[3])
                            },
                            'behavior_type': behavior_type,
                            'is_anomaly': self._is_anomaly_behavior(behavior_type)
                        }
                        results.append(result)
                
                # å†™å…¥è§†é¢‘å¸§ï¼ˆä¿®å¤å¸§æ ¼å¼é—®é¢˜ï¼‰
                if outputvideo and outputvideo.isOpened():
                    # ç¡®ä¿å¸§å°ºå¯¸æ­£ç¡®
                    if vis_img.shape[:2] != (height, width):
                        vis_img = cv2.resize(vis_img, (width, height))
                    
                    # ç¡®ä¿å¸§æ ¼å¼æ­£ç¡®ï¼ˆBGRï¼‰
                    if len(vis_img.shape) == 3 and vis_img.shape[2] == 3:
                        success = outputvideo.write(vis_img)
                        if not success:
                            print(f"âš  å†™å…¥è§†é¢‘å¸§å¤±è´¥: å¸§ {processed_frames}, å°ºå¯¸: {vis_img.shape}")
                    else:
                        print(f"âš  å¸§æ ¼å¼é”™è¯¯: {vis_img.shape}")
                        # è½¬æ¢ä¸ºBGRæ ¼å¼
                        if len(vis_img.shape) == 2:
                            vis_img = cv2.cvtColor(vis_img, cv2.COLOR_GRAY2BGR)
                        success = outputvideo.write(vis_img)
                
                # æ›´æ–°è¿›åº¦
                if progress_callback and total_frames > 0:
                    progress = (processed_frames / total_frames) * 100
                    progress_callback(task_id, progress)
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                with self.task_lock:
                    if task_id in self.current_tasks and self.current_tasks[task_id]['status'] == 'stopped':
                        break
            
            # æ¸…ç†èµ„æº
            cap.release()
            if outputvideo:
                outputvideo.release()
            
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
                if os.path.exists(config.output):
                    file_size = os.path.getsize(config.output)
                    print(f"âœ“ è§†é¢‘ä¿å­˜æˆåŠŸ: {config.output} ({file_size} bytes)")
                else:
                    print(f"âŒ è¾“å‡ºæ–‡ä»¶æœªç”Ÿæˆ: {config.output}")
            
        except Exception as e:
            print(f"æ£€æµ‹è¿‡ç¨‹é”™è¯¯: {e}")
            raise e
        
        return results
    
    def _run_realtime_detection(self, config, task_id: str, websocket_callback: callable = None):
        """
        æ‰§è¡Œå®æ—¶æ£€æµ‹çš„æ ¸å¿ƒé€»è¾‘
        """
        try:
            cap = MyVideoCapture(config.input)
            id_to_ava_labels = {}
            frame_count = 0
            
            while not cap.end:
                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                with self.task_lock:
                    if task_id in self.current_tasks and self.current_tasks[task_id]['status'] != 'running':
                        break
                
                ret, img = cap.read()
                if not ret:
                    continue
                
                frame_count += 1
                
                # YOLOæ£€æµ‹
                yolo_results = self.yolo_model.predict(
                    source=img, 
                    imgsz=config.imsize, 
                    device=config.device, 
                    verbose=False
                )
                boxes = yolo_results[0].boxes
                
                # å¤„ç†æ£€æµ‹ç»“æœ
                detections = []
                if len(boxes) > 0:
                    pred_xyxy = boxes.xyxy.cpu().numpy()
                    pred_conf = boxes.conf.cpu().numpy().reshape(-1, 1)
                    pred_cls = boxes.cls.cpu().numpy().reshape(-1, 1)
                    
                    pred = np.hstack((pred_xyxy, pred_conf, pred_cls))
                    xywh = np.hstack(((pred[:, 0:2] + pred[:, 2:4]) / 2, pred[:, 2:4] - pred[:, 0:2]))
                    
                    # DeepSortè·Ÿè¸ª
                    temp = deepsort_update(self.deepsort_tracker, pred, xywh, img)
                    temp = temp if len(temp) else np.ones((0, 8)).astype(np.float32)
                    
                    # æ ¼å¼åŒ–æ£€æµ‹ç»“æœ
                    for detection in temp:
                        if len(detection) >= 7:
                            detection_data = {
                                'frame_number': frame_count,
                                'timestamp': time.time(),
                                'object_id': int(detection[4]),
                                'object_type': 'person',
                                'confidence': float(detection[6]),
                                'bbox': {
                                    'x1': float(detection[0]),
                                    'y1': float(detection[1]),
                                    'x2': float(detection[2]),
                                    'y2': float(detection[3])
                                },
                                'behavior_type': id_to_ava_labels.get(int(detection[4]), 'unknown'),
                                'is_anomaly': False
                            }
                            detections.append(detection_data)
                    
                    # è¡Œä¸ºè¯†åˆ«ï¼ˆSlowFastï¼‰
                    if len(cap.stack) == 25:
                        clip = cap.get_video_clip()
                        if temp.shape[0] > 0:
                            try:
                                inputs, inp_boxes, _ = ava_inference_transform(clip, temp[:, 0:4], crop_size=config.imsize)
                                inp_boxes = torch.cat([torch.zeros(inp_boxes.shape[0], 1), inp_boxes], dim=1)
                                
                                if isinstance(inputs, list):
                                    inputs = [inp.unsqueeze(0).to(config.device) for inp in inputs]
                                else:
                                    inputs = inputs.unsqueeze(0).to(config.device)
                                
                                with torch.no_grad():
                                    slowfaster_preds = self.video_model(inputs, inp_boxes.to(config.device))
                                
                                for tid, avalabel in zip(temp[:, 5].tolist(), np.argmax(slowfaster_preds.cpu(), axis=1).tolist()):
                                    behavior = self.ava_labelnames[avalabel + 1]
                                    id_to_ava_labels[tid] = behavior
                                    
                                    # æ›´æ–°æ£€æµ‹ç»“æœä¸­çš„è¡Œä¸ºä¿¡æ¯
                                    for det in detections:
                                        if det['object_id'] == tid:
                                            det['behavior_type'] = behavior
                                            det['is_anomaly'] = self._is_anomaly_behavior(behavior)
                            except Exception as e:
                                print(f"å®æ—¶SlowFastå¤„ç†é”™è¯¯: {e}")
                
                # å‘é€å®æ—¶ç»“æœ
                if websocket_callback and detections:
                    websocket_callback({
                        'type': 'detection_result',
                        'task_id': task_id,
                        'frame_number': frame_count,
                        'timestamp': time.time(),
                        'detections': detections
                    })
                
                # æ£€æŸ¥å¼‚å¸¸è¡Œä¸ºå¹¶å‘é€æŠ¥è­¦
                for detection in detections:
                    if detection['is_anomaly'] and websocket_callback:
                        websocket_callback({
                            'type': 'alert',
                            'task_id': task_id,
                            'alert_type': detection['behavior_type'],
                            'detection': detection
                        })
                
                # æ§åˆ¶å¸§ç‡
                time.sleep(0.04)  # çº¦25 FPS
            
            # æ¸…ç†èµ„æº
            cap.release()
            
        except Exception as e:
            print(f"å®æ—¶æ£€æµ‹é”™è¯¯: {e}")
            raise e
    
    def _is_anomaly_behavior(self, behavior: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå¼‚å¸¸è¡Œä¸ºï¼ˆä½¿ç”¨ç²¾ç¡®æ˜ å°„ï¼Œæ’é™¤æ­£å¸¸è¡Œä¸ºï¼‰

        Args:
            behavior: è¡Œä¸ºåç§°ï¼ˆæ¥è‡ªAVAæ•°æ®é›†ï¼‰

        Returns:
            bool: æ˜¯å¦ä¸ºå¼‚å¸¸è¡Œä¸º
        """
        if not behavior:
            return False

        behavior_lower = behavior.lower().strip()

        # å¼ºåˆ¶æ’é™¤çš„æ­£å¸¸è¡Œä¸ºï¼ˆå³ä½¿ç”¨æˆ·é€‰æ‹©äº†ä¹Ÿä¸æŠ¥è­¦ï¼‰
        normal_behaviors = ['walk', 'sit', 'stand', 'run']
        if behavior_lower in normal_behaviors:
            print(f"â„¹ï¸ è¡Œä¸º '{behavior}' æ˜¯æ­£å¸¸è¡Œä¸ºï¼Œä¸è§¦å‘æŠ¥è­¦")
            return False

        print(f"ğŸ” å½“å‰æŠ¥è­¦è¡Œä¸ºé…ç½®: {self.alert_behaviors}")
        print(f"ğŸ” æ£€æµ‹åˆ°çš„è¡Œä¸º: '{behavior}'")

        # éå†ç”¨æˆ·é€‰æ‹©çš„æŠ¥è­¦è¡Œä¸º
        for alert_behavior in self.alert_behaviors:
            alert_behavior_lower = alert_behavior.lower().strip()

            # å†æ¬¡æ£€æŸ¥æ˜¯å¦ä¸ºæ­£å¸¸è¡Œä¸º
            if alert_behavior_lower in normal_behaviors:
                print(f"âš ï¸ è·³è¿‡æ­£å¸¸è¡Œä¸ºé…ç½®: '{alert_behavior}'")
                continue

            # è·å–è¯¥æŠ¥è­¦è¡Œä¸ºå¯¹åº”çš„AVAæ ‡ç­¾åˆ—è¡¨
            mapped_behaviors = self.behavior_mapping.get(alert_behavior_lower, [alert_behavior_lower])

            # æ£€æŸ¥å½“å‰æ£€æµ‹åˆ°çš„è¡Œä¸ºæ˜¯å¦åŒ¹é…ä»»ä½•æ˜ å°„çš„AVAæ ‡ç­¾
            for mapped_behavior in mapped_behaviors:
                mapped_behavior_lower = mapped_behavior.lower().strip()
                if behavior_lower == mapped_behavior_lower or mapped_behavior_lower in behavior_lower:
                    print(f"ğŸš¨ æ£€æµ‹åˆ°æŠ¥è­¦è¡Œä¸º: '{behavior}' åŒ¹é…ç”¨æˆ·è®¾ç½®çš„ '{alert_behavior}'")
                    return True

        # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
        print(f"â„¹ï¸ è¡Œä¸º '{behavior}' ä¸åœ¨æŠ¥è­¦åˆ—è¡¨ä¸­")
        return False
    
    def get_supported_behaviors(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„è¡Œä¸ºç±»å‹åˆ—è¡¨
        
        Returns:
            List[str]: è¡Œä¸ºç±»å‹åˆ—è¡¨
        """
        if self.ava_labelnames:
            return list(self.ava_labelnames.values())
        return []
    
    def update_config(self, new_config: Dict[str, Any]):
        """
        æ›´æ–°é…ç½®å‚æ•°
        
        Args:
            new_config: æ–°çš„é…ç½®å‚æ•°
        """
        self.config.update(new_config)
        
        # æ›´æ–°ç›¸å…³å‚æ•°
        if 'device' in new_config:
            self.device = new_config['device']
        if 'input_size' in new_config:
            self.input_size = new_config['input_size']
        if 'confidence_threshold' in new_config:
            self.confidence_threshold = new_config['confidence_threshold']
        if 'alert_behaviors' in new_config:
            self.alert_behaviors = new_config['alert_behaviors']


# å…¨å±€æ£€æµ‹æœåŠ¡å®ä¾‹
detection_service = None

def get_detection_service(config: Dict[str, Any] = None) -> BehaviorDetectionService:
    """
    è·å–æ£€æµ‹æœåŠ¡å®ä¾‹ï¼ˆæ”¯æŒé…ç½®æ›´æ–°ï¼‰

    Args:
        config: é…ç½®å‚æ•°

    Returns:
        BehaviorDetectionService: æ£€æµ‹æœåŠ¡å®ä¾‹
    """
    global detection_service

    # å¦‚æœæ²¡æœ‰é…ç½®å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    if config is None:
        config = {
            'device': 'cpu',
            'input_size': 640,
            'confidence_threshold': 0.5,
            'alert_behaviors': ['fall down', 'fight', 'enter'],  # é»˜è®¤åªå¯¹æœ€é‡è¦çš„ä¸‰ç§å¼‚å¸¸è¡Œä¸ºæŠ¥è­¦
            'output_format': 'both',
            'save_results': True
        }

    print(f"ğŸ”§ get_detection_service æ¥æ”¶åˆ°çš„é…ç½®: {config}")

    # å¦‚æœæœåŠ¡ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°å®ä¾‹
    if detection_service is None:
        print("ğŸ†• åˆ›å»ºæ–°çš„æ£€æµ‹æœåŠ¡å®ä¾‹")
        detection_service = BehaviorDetectionService(config)
    else:
        # å¦‚æœæœåŠ¡å·²å­˜åœ¨ï¼Œæ›´æ–°é…ç½®
        print("ğŸ”„ æ›´æ–°ç°æœ‰æ£€æµ‹æœåŠ¡é…ç½®")
        detection_service.device = config.get('device', detection_service.device)
        detection_service.input_size = config.get('input_size', detection_service.input_size)
        detection_service.confidence_threshold = config.get('confidence_threshold', detection_service.confidence_threshold)
        detection_service.alert_behaviors = config.get('alert_behaviors', detection_service.alert_behaviors)
        detection_service.output_format = config.get('output_format', detection_service.output_format)
        detection_service.save_results = config.get('save_results', detection_service.save_results)

        print(f"ğŸ”§ æ›´æ–°åçš„æ£€æµ‹æœåŠ¡é…ç½®:")
        print(f"   - è®¾å¤‡: {detection_service.device}")
        print(f"   - è¾“å…¥å°ºå¯¸: {detection_service.input_size}")
        print(f"   - ç½®ä¿¡åº¦é˜ˆå€¼: {detection_service.confidence_threshold}")
        print(f"   - æŠ¥è­¦è¡Œä¸º: {detection_service.alert_behaviors}")
        print(f"   - è¾“å‡ºæ ¼å¼: {detection_service.output_format}")
        print(f"   - ä¿å­˜ç»“æœ: {detection_service.save_results}")

    return detection_service