"""
æ£€æµ‹æœåŠ¡æ¨¡å— - å°è£…YOLOv8+SlowFastç®—æ³•
"""
import os
import sys
import cv2
import time
import json
import threading
import queue
from datetime import datetime
import base64
from typing import Dict, List, Optional, Tuple, Any

# æ·»åŠ ç®—æ³•æ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
yolo_slowfast_path = os.path.join(project_root, 'yolo_slowfast-master')
sys.path.append(yolo_slowfast_path)

# å¯¼å…¥ç°æœ‰ç®—æ³•æ¨¡å—
try:
    from yolo_slowfast import *
    import torch
    import numpy as np
    from ultralytics import YOLO
    from pytorchvideo.data.ava import AvaLabeledVideoFramePaths
    from pytorchvideo.models.hub import slowfast_r50_detection
    from deep_sort.deep_sort import DeepSort
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥ç®—æ³•æ¨¡å—: {e}")


class BehaviorDetectionService:
    """è¡Œä¸ºæ£€æµ‹æœåŠ¡ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ£€æµ‹æœåŠ¡
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«è®¾å¤‡ã€è¾“å…¥å°ºå¯¸ã€ç½®ä¿¡åº¦é˜ˆå€¼ç­‰å‚æ•°
        """
        # è®¾å¤‡é…ç½® - æ™ºèƒ½GPUæ£€æµ‹ï¼Œé»˜è®¤ä¼˜å…ˆä½¿ç”¨GPU
        device_config = config.get('device', 'auto').lower()

        if device_config == 'auto':
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡ - ä¼˜å…ˆGPU
            if torch.cuda.is_available():
                self.device = 'cuda'
                print(f"âœ“ è‡ªåŠ¨é€‰æ‹©GPU: {torch.cuda.get_device_name()}")
            else:
                self.device = 'cpu'
                print("âœ“ è‡ªåŠ¨é€‰æ‹©CPU (GPUä¸å¯ç”¨)")
        elif device_config == 'cuda':
            if torch.cuda.is_available():
                self.device = 'cuda'
                print(f"âœ“ å¼ºåˆ¶ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            else:
                self.device = 'cpu'
                print("âš  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
        else:
            self.device = 'cpu'
            print("âœ“ ä½¿ç”¨CPU")
        
        # åŠ è½½COCOç±»åˆ«åç§°
        coco_names_path = os.path.join(yolo_slowfast_path, 'selfutils', 'coco_names.txt')
        self.coco_names = []
        if os.path.exists(coco_names_path):
            with open(coco_names_path, 'r') as f:
                self.coco_names = [line.strip() for line in f.readlines()]
            print(f"âœ“ åŠ è½½COCOç±»åˆ«åç§°: {len(self.coco_names)}ä¸ªç±»åˆ«")
        else:
            print("âš  COCOç±»åˆ«åç§°æ–‡ä»¶ä¸å­˜åœ¨")
            # ä½¿ç”¨é»˜è®¤ç±»åˆ«
            self.coco_names = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane']
        
        self.input_size = config.get('input_size', 640)
        self.confidence_threshold = config.get('confidence_threshold', 0.5)
        
        # åˆå§‹åŒ–æ ‡å¿—
        self.models_initialized = False
        self.task_lock = threading.Lock()
        self.stopped_tasks = set()
        self.current_tasks = {}
        self.should_stop_realtime = False  # åœæ­¢å®æ—¶ç›‘æ§çš„æ ‡å¿—ï¼ˆå¯¹åº”æ ‡å‡†å®ç°çš„should_stopï¼‰
        self.is_running = False  # æ£€æµ‹å™¨è¿è¡ŒçŠ¶æ€æ ‡å¿—ï¼ˆæŒ‰ç…§æ ‡å‡†å®ç°æ·»åŠ ï¼‰
        self.stop_event = threading.Event()  # æ·»åŠ åœæ­¢äº‹ä»¶å¯¹è±¡
        self.active_streams = {}  # æ´»è·ƒæµè·Ÿè¸ª
        
        # æ¨¡å‹ç›¸å…³è·¯å¾„
        self.yolo_model_path = 'yolov8n.pt'
        self.slowfast_weights_path = 'SLOWFAST_8x8_R50_DETECTION.pyth'
        self.deepsort_weights_path = 'ckpt.t7'
        self.ava_labels_path = 'temp.pbtxt'
        
        # æ¨¡å‹å¯¹è±¡
        self.yolo_model = None
        self.video_model = None
        self.deepsort_tracker = None
        self.ava_labelnames = None
        
        # æŠ¥è­¦é…ç½®
        self.alert_behaviors = config.get('alert_behaviors', ['fall down', 'fight', 'enter', 'exit'])
        
    def initialize_models(self) -> bool:
        """
        åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            print("æ­£åœ¨åˆå§‹åŒ–ç®—æ³•æ¨¡å‹...")
            
            # åˆ‡æ¢åˆ°ç®—æ³•ç›®å½•
            original_cwd = os.getcwd()
            os.chdir(yolo_slowfast_path)
            
            # åˆå§‹åŒ–YOLOæ¨¡å‹
            self.yolo_model = YOLO(self.yolo_model_path)
            print(f"âœ“ YOLOæ¨¡å‹å·²åŠ è½½: {self.yolo_model_path}")
            
            # åˆå§‹åŒ–SlowFastæ¨¡å‹
            if os.path.exists(self.slowfast_weights_path):
                self.video_model = slowfast_r50_detection(False)
                checkpoint = torch.load(self.slowfast_weights_path, map_location=self.device)
                self.video_model.load_state_dict(checkpoint['model_state'])
                self.video_model = self.video_model.eval().to(self.device)
                print(f"âœ“ SlowFastæ¨¡å‹å·²åŠ è½½: {self.slowfast_weights_path}")
            else:
                print(f"âš  SlowFastæƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {self.slowfast_weights_path}")
                self.video_model = slowfast_r50_detection(True).eval().to(self.device)
            
            # åˆå§‹åŒ–DeepSortè·Ÿè¸ªå™¨
            if os.path.exists(self.deepsort_weights_path):
                self.deepsort_tracker = DeepSort(self.deepsort_weights_path)
                print(f"âœ“ DeepSortè·Ÿè¸ªå™¨å·²åŠ è½½: {self.deepsort_weights_path}")
            else:
                # å¦‚æœç»å¯¹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ç›¸å¯¹è·¯å¾„
                relative_path = "deep_sort/deep_sort/deep/checkpoint/ckpt.t7"
                if os.path.exists(relative_path):
                    self.deepsort_tracker = DeepSort(relative_path)
                    print(f"âœ“ DeepSortè·Ÿè¸ªå™¨å·²åŠ è½½: {relative_path}")
                else:
                    print(f"âš  DeepSortæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.deepsort_weights_path}")
                    print(f"âš  ç›¸å¯¹è·¯å¾„ä¹Ÿä¸å­˜åœ¨: {relative_path}")
                    return False
            
            # åŠ è½½AVAæ ‡ç­¾
            if os.path.exists(self.ava_labels_path):
                self.ava_labelnames, _ = AvaLabeledVideoFramePaths.read_label_map(self.ava_labels_path)
                print(f"âœ“ AVAæ ‡ç­¾å·²åŠ è½½: {self.ava_labels_path}")
            else:
                print(f"âš  AVAæ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {self.ava_labels_path}")
                return False
            
            # åˆå§‹åŒ–é¢œè‰²æ˜ å°„
            self.color_map = [[random.randint(0, 255) for _ in range(3)] for _ in range(80)]
            
            # æ¢å¤åŸå§‹ç›®å½•
            os.chdir(original_cwd)
            
            self.models_initialized = True
            print("âœ“ æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # æ¢å¤åŸå§‹ç›®å½•
            try:
                os.chdir(original_cwd)
            except:
                pass
            return False
    
    def detect_video(self, video_path: str, output_path: str = None, 
                    progress_callback: callable = None) -> Dict[str, Any]:
        """
        æ£€æµ‹è§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            Dict: æ£€æµ‹ç»“æœ
        """
        if not self.models_initialized:
            if not self.initialize_models():
                return {'success': False, 'error': 'æ¨¡å‹åˆå§‹åŒ–å¤±è´¥'}
        
        try:
            # åˆ›å»ºä»»åŠ¡ID
            task_id = f"video_{int(time.time())}"
            
            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆåœ¨åˆ‡æ¢ç›®å½•å‰ï¼‰
            video_path = os.path.abspath(video_path)
            if output_path:
                output_path = os.path.abspath(output_path)
            
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(video_path):
                return {'success': False, 'error': f'è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}'}
            
            # åˆ‡æ¢åˆ°ç®—æ³•ç›®å½•
            original_cwd = os.getcwd()
            os.chdir(yolo_slowfast_path)
            
            # å‡†å¤‡æ£€æµ‹å‚æ•°
            config = type('Config', (), {})()
            config.input = video_path
            config.output = output_path or ''
            config.imsize = self.input_size
            config.device = self.device
            config.show = False
            config.conf = self.confidence_threshold
            config.iou = 0.4
            config.classes = None
            
            # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
            with self.task_lock:
                self.current_tasks[task_id] = {
                    'type': 'video',
                    'status': 'running',
                    'start_time': time.time(),
                    'progress': 0.0
                }
            
            # æ‰§è¡Œæ£€æµ‹
            results = self._run_detection(config, task_id, progress_callback)
            
            # æ¢å¤åŸå§‹ç›®å½•
            os.chdir(original_cwd)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            with self.task_lock:
                if task_id in self.current_tasks:
                    self.current_tasks[task_id]['status'] = 'completed'
                    self.current_tasks[task_id]['end_time'] = time.time()
            
            return {
                'success': True,
                'task_id': task_id,
                'results': results,
                'output_path': output_path
            }
            
        except Exception as e:
            # æ¢å¤åŸå§‹ç›®å½•
            try:
                os.chdir(original_cwd)
            except:
                pass
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            with self.task_lock:
                if task_id in self.current_tasks:
                    self.current_tasks[task_id]['status'] = 'failed'
                    self.current_tasks[task_id]['error'] = str(e)
            
            return {'success': False, 'error': str(e)}
    
    def start_realtime_detection(self, source: int = 0, 
                                websocket_callback: callable = None) -> str:
        """
        å¯åŠ¨å®æ—¶æ£€æµ‹
        
        Args:
            source: æ‘„åƒå¤´ID
            websocket_callback: WebSocketå›è°ƒå‡½æ•°
            
        Returns:
            str: ä»»åŠ¡ID
        """
        if not self.models_initialized:
            if not self.initialize_models():
                raise Exception('æ¨¡å‹åˆå§‹åŒ–å¤±è´¥')
        
        task_id = f"realtime_{int(time.time())}"
        
        def realtime_worker():
            try:
                # åˆ‡æ¢åˆ°ç®—æ³•ç›®å½•
                original_cwd = os.getcwd()
                os.chdir(yolo_slowfast_path)
                
                # å‡†å¤‡æ£€æµ‹å‚æ•°
                config = type('Config', (), {})()
                config.input = source
                config.output = ''
                config.imsize = self.input_size
                config.device = self.device
                config.show = False
                config.conf = self.confidence_threshold
                config.iou = 0.4
                config.classes = None
                
                # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
                with self.task_lock:
                    self.current_tasks[task_id] = {
                        'type': 'realtime',
                        'status': 'running',
                        'start_time': time.time(),
                        'source': source
                    }
                
                # æ‰§è¡Œå®æ—¶æ£€æµ‹
                self._run_realtime_detection(config, task_id, websocket_callback)
                
                # æ¢å¤åŸå§‹ç›®å½•
                os.chdir(original_cwd)
                
            except Exception as e:
                print(f"å®æ—¶æ£€æµ‹é”™è¯¯: {e}")
                # æ¢å¤åŸå§‹ç›®å½•
                try:
                    os.chdir(original_cwd)
                except:
                    pass
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                with self.task_lock:
                    if task_id in self.current_tasks:
                        self.current_tasks[task_id]['status'] = 'failed'
                        self.current_tasks[task_id]['error'] = str(e)
        
        # å¯åŠ¨å®æ—¶æ£€æµ‹çº¿ç¨‹
        thread = threading.Thread(target=realtime_worker, daemon=True)
        thread.start()
        
        return task_id

    def generate_realtime_frames(self, source: Any, preview_only: bool = False):
        """
        ç”Ÿæˆå®æ—¶è§†é¢‘å¸§æµï¼Œç”¨äºHTTPè§†é¢‘æµä¼ è¾“
        è¿™æ˜¯ä» behavior_identify é¡¹ç›®è¿ç§»çš„åŠŸèƒ½

        Args:
            source: è§†é¢‘æºï¼ˆæ‘„åƒå¤´IDæˆ–è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼‰
            preview_only: æ˜¯å¦ä»…é¢„è§ˆæ¨¡å¼ï¼ˆä¸è¿›è¡Œè¡Œä¸ºæ£€æµ‹ï¼‰

        Yields:
            bytes: JPEGæ ¼å¼çš„è§†é¢‘å¸§æ•°æ®
        """
        mode_text = "ä»…é¢„è§ˆ" if preview_only else "å®æ—¶æ£€æµ‹"
        print(f"ğŸ¥ å¼€å§‹ç”Ÿæˆå®æ—¶è§†é¢‘å¸§æµï¼Œè§†é¢‘æº: {source}ï¼Œæ¨¡å¼: {mode_text}")

        # æŒ‰ç…§æ ‡å‡†å®ç°ï¼šå¼€å§‹æ–°ä¼šè¯æ—¶é‡ç½®åœæ­¢æ ‡å¿—
        self.should_stop_realtime = False  # é‡ç½®åœæ­¢æ ‡å¿—ï¼Œå¼€å§‹æ–°çš„ç›‘æ§ä¼šè¯
        self.stop_event.clear()  # æ¸…é™¤åœæ­¢äº‹ä»¶
        self.is_running = True  # è®¾ç½®è¿è¡ŒçŠ¶æ€
        print(f"ğŸ¥ å¼€å§‹æ–°ç›‘æ§ä¼šè¯ - should_stop: {self.should_stop_realtime}, is_running: {self.is_running}")

        if not self.models_initialized:
            print("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œå°è¯•åˆå§‹åŒ–...")
            if not self.initialize_models():
                print("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆè§†é¢‘å¸§")
                return

        try:
            # åˆ‡æ¢åˆ°ç®—æ³•ç›®å½•
            original_cwd = os.getcwd()
            os.chdir(yolo_slowfast_path)

            # ç¡®ä¿å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from yolo_slowfast import MyVideoCapture, ava_inference_transform, deepsort_update, plot_one_box

            # å¤„ç†è§†é¢‘æºå‚æ•°
            if source == '0' or source == 0:
                source = 0  # æ‘„åƒå¤´
            elif isinstance(source, str) and source.isdigit():
                source = int(source)  # æ‘„åƒå¤´ID

            print(f"å¤„ç†åçš„è§†é¢‘æº: {source}, ç±»å‹: {type(source)}")

            # åˆå§‹åŒ–è§†é¢‘æ•è·
            cap = MyVideoCapture(source)
            id_to_ava_labels = {}

            # é¢œè‰²æ˜ å°„
            import random
            coco_color_map = [[random.randint(0, 255) for _ in range(3)] for _ in range(80)]

            # clip é˜Ÿåˆ—å’ŒåŠ¨ä½œè¯†åˆ«çº¿ç¨‹
            clip_queue = queue.Queue()
            result_queue = queue.Queue()

            def slowfast_worker():
                # ä»…å½“ä½¿ç”¨GPUæ—¶æ‰åˆ›å»ºç‹¬ç«‹çš„CUDAæµ
                import contextlib
                stream = torch.cuda.Stream() if 'cuda' in str(self.device) else None
                context_manager = torch.cuda.stream(stream) if stream else contextlib.nullcontext()

                while True:
                    # æ£€æŸ¥åœæ­¢ä¿¡å·
                    if self.should_stop_realtime:
                        print("SlowFast workeræ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                        break

                    try:
                        # ä½¿ç”¨è¶…æ—¶è·å–ä»»åŠ¡ï¼Œé¿å…æ— é™ç­‰å¾…
                        item = clip_queue.get(timeout=1.0)
                        if item is None:
                            break
                    except:
                        # è¶…æ—¶æˆ–å…¶ä»–å¼‚å¸¸ï¼Œç»§ç»­æ£€æŸ¥åœæ­¢ä¿¡å·
                        continue

                    idx, clip, pred_result = item

                    # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
                    if self.should_stop_realtime:
                        print("SlowFast workeråœ¨å¤„ç†å‰æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                        clip_queue.task_done()
                        break

                    with context_manager:
                        if pred_result.pred[0].shape[0]:
                            inputs, inp_boxes, _ = ava_inference_transform(clip, pred_result.pred[0][:, 0:4], crop_size=self.input_size)
                            inp_boxes = torch.cat([torch.zeros(inp_boxes.shape[0], 1), inp_boxes], dim=1)
                            if isinstance(inputs, list):
                                inputs = [inp.unsqueeze(0).to(self.device, non_blocking=True) for inp in inputs]
                            else:
                                inputs = inputs.unsqueeze(0).to(self.device, non_blocking=True)

                            inp_boxes_gpu = inp_boxes.to(self.device, non_blocking=True)

                            with torch.no_grad():
                                slowfaster_preds = self.video_model(inputs, inp_boxes_gpu)

                            # ä¿®å¤æ•°æ®ç±»å‹è½¬æ¢é—®é¢˜ - ç¡®ä¿æ­£ç¡®çš„æ•°æ®ç±»å‹è½¬æ¢
                            slowfaster_preds_cpu = slowfaster_preds.cpu().float()  # ç¡®ä¿ä¸ºfloatç±»å‹
                            pred_labels = torch.argmax(slowfaster_preds_cpu, dim=1).numpy().astype(np.int32)
                            track_ids = pred_result.pred[0][:, 5].astype(np.int32)

                            result_queue.put((idx, track_ids.tolist(), pred_labels.tolist()))
                    clip_queue.task_done()

                print("SlowFast workerçº¿ç¨‹å·²é€€å‡º")

            # å¯åŠ¨åŠ¨ä½œè¯†åˆ«å·¥ä½œçº¿ç¨‹
            threading.Thread(target=slowfast_worker, daemon=True).start()

            # ä¸»å¤„ç†å¾ªç¯ - æŒ‰ç…§æ ‡å‡†å®ç°é€»è¾‘ï¼ˆç®€åŒ–å¾ªç¯æ¡ä»¶ï¼‰
            frame_count = 0
            print(f"ğŸ¥ å¼€å§‹ä¸»å¤„ç†å¾ªç¯")
            while not cap.end and not self.should_stop_realtime:
                frame_count += 1
                # æ¯100å¸§æ‰“å°ä¸€æ¬¡çŠ¶æ€
                if frame_count % 100 == 0:
                    print(f"ğŸ¥ å¤„ç†ç¬¬{frame_count}å¸§ - should_stop: {self.should_stop_realtime}, event_set: {self.stop_event.is_set()}")

                # åœ¨å¾ªç¯å¼€å§‹æ—¶æ£€æŸ¥åœæ­¢æ ‡å¿—ï¼ˆæŒ‰ç…§æ ‡å‡†å®ç°ï¼‰
                if self.should_stop_realtime:
                    print(f"ğŸ¥ åœ¨ç¬¬{frame_count}å¸§æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡ºå®æ—¶ç›‘æ§...")
                    print(f"ğŸ¥ åœæ­¢æ ‡å¿—çŠ¶æ€: should_stop={self.should_stop_realtime}, is_running={self.is_running}")
                    break

                ret, img = cap.read()
                if not ret:
                    # å¦‚æœè¯»å–å¤±è´¥ï¼Œä¹Ÿæ£€æŸ¥åœæ­¢æ ‡å¿—
                    if self.should_stop_realtime:
                        print("ğŸ¥ è¯»å–å¤±è´¥æ—¶æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                        break
                    continue

                # å†æ¬¡æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢ï¼ˆæŒ‰ç…§æ ‡å‡†å®ç°ï¼‰
                if self.should_stop_realtime:
                    print("ğŸ¥ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡ºå®æ—¶ç›‘æ§...")
                    break

                # ğŸ”§ é¢„è§ˆæ¨¡å¼ï¼šè·³è¿‡å¤æ‚çš„æ£€æµ‹é€»è¾‘ï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹ç”»é¢
                if preview_only:
                    # é¢„è§ˆæ¨¡å¼ï¼šåªæ˜¾ç¤ºåŸå§‹æ‘„åƒå¤´ç”»é¢ï¼Œä¸è¿›è¡Œä»»ä½•æ£€æµ‹
                    pass  # imgä¿æŒåŸå§‹çŠ¶æ€
                else:
                    # å®æ—¶æ£€æµ‹æ¨¡å¼ï¼šæ‰§è¡Œå®Œæ•´çš„YOLO + SlowFastæ£€æµ‹
                    # YOLOæ£€æµ‹
                    results = self.yolo_model.predict(source=img, imgsz=self.input_size, device=self.device, verbose=False)
                    boxes = results[0].boxes  # YOLOv8 Results object

                    # å¤„ç†YOLOæ£€æµ‹ç»“æœ
                    if boxes is not None and len(boxes) > 0:
                        # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
                        if self.should_stop_realtime:
                            print("åœ¨YOLOå¤„ç†é˜¶æ®µæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                            break

                        pred_xyxy = boxes.xyxy.cpu().numpy()
                        pred_conf = boxes.conf.cpu().numpy().reshape(-1, 1)
                        pred_cls = boxes.cls.cpu().numpy().reshape(-1, 1)

                        pred = np.hstack((pred_xyxy, pred_conf, pred_cls))
                        xywh = np.hstack(((pred[:, 0:2] + pred[:, 2:4]) / 2, pred[:, 2:4] - pred[:, 0:2]))

                        # DeepSortè·Ÿè¸ª
                        temp = deepsort_update(self.deepsort_tracker, pred, xywh, img)
                        temp = temp if len(temp) else np.ones((0, 8)).astype(np.float32)

                        # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
                        if self.should_stop_realtime:
                            print("åœ¨DeepSortå¤„ç†é˜¶æ®µæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                            break

                        # æ ¼å¼åŒ–æ£€æµ‹ç»“æœ
                        pred_result = type("YoloPred", (), {})()
                        pred_result.ims = [img]
                        pred_result.pred = [temp.astype(np.float32)]
                        pred_result.names = self.yolo_model.names

                        # è¡Œä¸ºè¯†åˆ«ï¼ˆSlowFastï¼‰ - å½“ç§¯ç´¯äº†25å¸§æ—¶
                        if len(cap.stack) == 25:
                            clip = cap.get_video_clip()
                            clip_queue.put((cap.idx, clip, pred_result))

                        # å¤„ç†åŠ¨ä½œè¯†åˆ«ç»“æœ
                        while not result_queue.empty():
                            try:
                                _, tids, avalabels = result_queue.get_nowait()
                                for tid, avalabel in zip(tids, avalabels):
                                    id_to_ava_labels[tid] = self.ava_labelnames[avalabel + 1]
                            except queue.Empty:
                                break

                        # å†æ¬¡æ£€æŸ¥åœæ­¢ä¿¡å·
                        if self.should_stop_realtime:
                            print("åœ¨ç»“æœå¤„ç†é˜¶æ®µæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                            break

                        # ç»˜åˆ¶æ£€æµ‹ç»“æœ - ä½¿ç”¨ä¸behavior_identifyç›¸åŒçš„é€»è¾‘
                        annotated_frame = img.copy()
                        for _, pred in enumerate(pred_result.pred):
                            if pred.shape[0]:
                                for _, (*box, cls, trackid, _, _) in enumerate(pred):
                                    if int(cls) != 0:
                                        ava_label = ''
                                    elif trackid in id_to_ava_labels.keys():
                                        ava_label = id_to_ava_labels[trackid].split(' ')[0]
                                    else:
                                        ava_label = 'Unknown'
                                    text = '{} {} {}'.format(int(trackid), pred_result.names[int(cls)], ava_label)
                                    color = coco_color_map[int(cls)]
                                    annotated_frame = plot_one_box(box, annotated_frame, color, text)

                        # ä½¿ç”¨ç»˜åˆ¶åçš„å¸§
                        img = annotated_frame

                # åœ¨å‘é€å¸§ä¹‹å‰æœ€åä¸€æ¬¡æ£€æŸ¥åœæ­¢æ ‡å¿—ï¼ˆæŒ‰ç…§æ ‡å‡†å®ç°ï¼‰
                if self.should_stop_realtime:
                    print("ğŸ¥ åœ¨å‘é€å¸§å‰æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                    return  # ç›´æ¥è¿”å›ï¼Œç»“æŸç”Ÿæˆå™¨

                # ç¼–ç ä¸ºJPEG
                ret, buffer = cv2.imencode('.jpg', img)
                if ret:
                    frame = buffer.tobytes()

                    # åœ¨yieldå‰å†æ¬¡æ£€æŸ¥åœæ­¢æ ‡å¿—
                    if self.should_stop_realtime:
                        print("ğŸ¥ åœ¨yieldå‰æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                        return

                    yield (b'--frame\r\n'
                           b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')

                    # yieldåç«‹å³æ£€æŸ¥åœæ­¢æ ‡å¿—
                    if self.should_stop_realtime:
                        print("ğŸ¥ åœ¨yieldåæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                        return

                # æ§åˆ¶å¸§ç‡ - åœ¨sleepæœŸé—´ä¹Ÿæ£€æŸ¥åœæ­¢æ ‡å¿—ï¼ˆæŒ‰ç…§æ ‡å‡†å®ç°ï¼‰
                for i in range(33):  # åˆ†è§£sleepä¸ºå¤šä¸ªå°é—´éš”ï¼Œä¾¿äºå¿«é€Ÿå“åº”åœæ­¢ä¿¡å·
                    if self.should_stop_realtime:
                        print(f"ğŸ¥ åœ¨å¸§ç‡æ§åˆ¶æœŸé—´({i}/33)æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                        return  # ç›´æ¥è¿”å›ï¼Œç»“æŸç”Ÿæˆå™¨
                    # ä½¿ç”¨__import__ç¡®ä¿è·å–æ­£ç¡®çš„timeæ¨¡å—
                    __import__('time').sleep(0.001)  # 1ms * 33 = 33ms â‰ˆ 30FPS

        except Exception as e:
            print(f"ğŸ¥ ç”Ÿæˆè§†é¢‘å¸§æ—¶å‡ºé”™: {e}")
        finally:
            # æŒ‰ç…§æ ‡å‡†å®ç°è¿›è¡Œèµ„æºæ¸…ç†
            print("ğŸ¥ æ­£åœ¨æ¸…ç†èµ„æº...")
            self.is_running = False  # æŒ‰ç…§æ ‡å‡†å®ç°é‡ç½®è¿è¡ŒçŠ¶æ€

            # å¼ºåˆ¶é‡Šæ”¾æ‘„åƒå¤´èµ„æº
            try:
                if 'clip_queue' in locals():
                    clip_queue.put(None)  # åœæ­¢å·¥ä½œçº¿ç¨‹
                    print("ğŸ¥ å·¥ä½œçº¿ç¨‹åœæ­¢ä¿¡å·å·²å‘é€")
            except Exception as e:
                print(f"ğŸ¥ åœæ­¢å·¥ä½œçº¿ç¨‹æ—¶å‡ºé”™: {e}")

            try:
                if 'cap' in locals() and cap is not None:
                    print(f"ğŸ¥ é‡Šæ”¾æ‘„åƒå¤´èµ„æº...")
                    cap.release()  # é‡Šæ”¾è§†é¢‘æ•è·èµ„æº
                    print("ğŸ¥ æ‘„åƒå¤´èµ„æºå·²é‡Šæ”¾")

                    # å¼ºåˆ¶ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿èµ„æºå®Œå…¨é‡Šæ”¾
                    time.sleep(0.2)
                    print("ğŸ¥ æ‘„åƒå¤´èµ„æºé‡Šæ”¾å®Œæˆ")
                else:
                    print("ğŸ¥ è­¦å‘Šï¼šæ‘„åƒå¤´å¯¹è±¡ä¸å­˜åœ¨æˆ–å·²ä¸ºNone")
            except Exception as cleanup_error:
                print(f"ğŸ¥ é‡Šæ”¾æ‘„åƒå¤´èµ„æºæ—¶å‡ºé”™: {cleanup_error}")

            try:
                if 'original_cwd' in locals():
                    os.chdir(original_cwd)
                    print("ğŸ¥ å·¥ä½œç›®å½•å·²æ¢å¤")
            except Exception as e:
                print(f"ğŸ¥ æ¢å¤å·¥ä½œç›®å½•æ—¶å‡ºé”™: {e}")

            print("ğŸ¥ æ£€æµ‹å™¨å·²åœæ­¢")
            print(f"ğŸ¥ æœ€ç»ˆçŠ¶æ€ - should_stop: {self.should_stop_realtime}, is_running: {self.is_running}")

    def stop_realtime_detection(self, task_id: str) -> bool:
        """
        åœæ­¢å®æ—¶æ£€æµ‹

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸåœæ­¢
        """
        with self.task_lock:
            if task_id in self.current_tasks:
                self.current_tasks[task_id]['status'] = 'stopped'
                return True
        return False

    def stop_realtime_monitoring(self):
        """åœæ­¢æ‰€æœ‰å®æ—¶ç›‘æ§ - æŒ‰ç…§æ ‡å‡†å®ç°é€»è¾‘"""
        print("ğŸ›‘ SERVICE: Stopping monitoring...")
        print(f"ğŸ›‘ åœæ­¢å‰çŠ¶æ€ - should_stop: {self.should_stop_realtime}, is_running: {self.is_running}")

        # æŒ‰ç…§æ ‡å‡†å®ç°è®¾ç½®çŠ¶æ€æ ‡å¿—
        self.should_stop_realtime = True  # å¯¹åº”æ ‡å‡†å®ç°çš„should_stop
        self.is_running = False  # æŒ‰ç…§æ ‡å‡†å®ç°è®¾ç½®è¿è¡ŒçŠ¶æ€
        self.stop_event.set()  # è®¾ç½®åœæ­¢äº‹ä»¶

        print(f"ğŸ›‘ åœæ­¢åçŠ¶æ€ - should_stop: {self.should_stop_realtime}, is_running: {self.is_running}")

        # å¼ºåˆ¶ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿ç”Ÿæˆå™¨æœ‰æœºä¼šæ£€æŸ¥åœæ­¢æ ‡å¿—
        time.sleep(0.1)
        print("ğŸ›‘ åœæ­¢ä¿¡å·å·²å‘é€ï¼Œç­‰å¾…ç”Ÿæˆå™¨å“åº”...")

        # åœæ­¢æ‰€æœ‰å½“å‰ä»»åŠ¡
        with self.task_lock:
            for task_id in list(self.current_tasks.keys()):
                if self.current_tasks[task_id]['status'] == 'running':
                    self.current_tasks[task_id]['status'] = 'stopped'
                    print(f"ğŸ›‘ åœæ­¢ä»»åŠ¡: {task_id}")

        # æ˜¾ç¤ºæ´»è·ƒæµä¿¡æ¯
        print(f"ğŸ›‘ å½“å‰æ´»è·ƒæµæ•°é‡: {len(self.active_streams)}")
        for stream_id, stream_info in self.active_streams.items():
            print(f"ğŸ›‘ æ´»è·ƒæµ: {stream_id} - æ‘„åƒå¤´: {stream_info['camera_id']}")

        # ğŸ”§ æ–°å¢ï¼šå¼ºåˆ¶é‡Šæ”¾æ‰€æœ‰æ‘„åƒå¤´èµ„æº
        self._force_release_cameras()

        print("ğŸ›‘ SERVICE: Monitoring stopped successfully.")

    def _force_release_cameras(self):
        """å¼ºåˆ¶é‡Šæ”¾æ‰€æœ‰æ‘„åƒå¤´èµ„æº"""
        print("ğŸ¥ å¼ºåˆ¶é‡Šæ”¾æ‘„åƒå¤´èµ„æº...")
        try:
            # ä½¿ç”¨OpenCVå¼ºåˆ¶é‡Šæ”¾æ‰€æœ‰æ‘„åƒå¤´
            import cv2

            # å°è¯•é‡Šæ”¾å¸¸ç”¨çš„æ‘„åƒå¤´ç´¢å¼•
            for camera_id in range(1):  # æ£€æŸ¥æ‘„åƒå¤´0
                try:
                    temp_cap = cv2.VideoCapture(camera_id)
                    if temp_cap.isOpened():
                        print(f"ğŸ¥ å‘ç°æ´»è·ƒæ‘„åƒå¤´ {camera_id}ï¼Œæ­£åœ¨é‡Šæ”¾...")
                        temp_cap.release()
                        print(f"ğŸ¥ æ‘„åƒå¤´ {camera_id} å·²é‡Šæ”¾")
                    temp_cap = None
                except Exception as e:
                    print(f"ğŸ¥ é‡Šæ”¾æ‘„åƒå¤´ {camera_id} æ—¶å‡ºé”™: {e}")

            # é¢å¤–ç­‰å¾…æ—¶é—´ç¡®ä¿èµ„æºå®Œå…¨é‡Šæ”¾
            time.sleep(0.3)
            print("ğŸ¥ æ‘„åƒå¤´å¼ºåˆ¶é‡Šæ”¾å®Œæˆ")

        except Exception as e:
            print(f"ğŸ¥ å¼ºåˆ¶é‡Šæ”¾æ‘„åƒå¤´æ—¶å‡ºé”™: {e}")

    def stop_monitoring(self):
        """åœæ­¢å®æ—¶ç›‘æ§ - æ ‡å‡†æ¥å£ï¼ˆæŒ‰ç…§åˆ†ææ–‡æ¡£çš„æ ‡å‡†å®ç°ï¼‰"""
        print("ğŸ›‘ SERVICE: Stopping monitoring...")
        print(f"ğŸ›‘ å½“å‰çŠ¶æ€æ£€æŸ¥ - is_running: {getattr(self, 'is_running', False)}, should_stop: {getattr(self, 'should_stop_realtime', False)}")

        # æ— è®ºæ˜¯å¦æœ‰æ´»è·ƒæ£€æµ‹å™¨ï¼Œéƒ½å‘é€åœæ­¢ä¿¡å·
        self.stop_realtime_monitoring()  # è°ƒç”¨å…·ä½“çš„åœæ­¢é€»è¾‘
        print("ğŸ›‘ SERVICE: Stop signal sent.")

    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            Dict: ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
        """
        with self.task_lock:
            return self.current_tasks.get(task_id, {'status': 'not_found'})
    
    def _run_detection(self, config, task_id: str, progress_callback: callable = None) -> List[Dict]:
        """
        æ‰§è¡Œæ£€æµ‹çš„æ ¸å¿ƒé€»è¾‘ï¼ˆåŸºäºç°æœ‰ç®—æ³•ï¼‰
        """
        results = []
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„mainå‡½æ•°é€»è¾‘ï¼Œä½†è¿›è¡Œäº†ä¿®æ”¹ä»¥æ”¯æŒå›è°ƒ
            cap = MyVideoCapture(config.input)
            id_to_ava_labels = {}
            
            total_frames = int(cap.cap.get(cv2.CAP_PROP_FRAME_COUNT))
            processed_frames = 0
            
            # è®¾ç½®è¾“å‡ºè§†é¢‘ - ä¿®å¤ç¼–è§£ç å™¨é—®é¢˜
            outputvideo = None
            if config.output:
                video = cv2.VideoCapture(config.input)
                width, height = int(video.get(3)), int(video.get(4))
                fps = int(video.get(cv2.CAP_PROP_FPS)) or 25
                video.release()
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(config.output), exist_ok=True)
                
                # ä½¿ç”¨æµè§ˆå™¨å…¼å®¹çš„MP4æ ¼å¼ï¼Œä¼˜å…ˆå°è¯•H.264ç¼–è§£ç å™¨
                output_mp4 = config.output.replace('.avi', '.mp4')
                
                # å°è¯•ä¸åŒçš„ç¼–è§£ç å™¨ï¼Œä¼˜å…ˆä½¿ç”¨æµè§ˆå™¨å…¼å®¹æ€§æœ€å¥½çš„
                codecs_to_try = [
                    ('avc1', 'H.264 (æœ€ä½³æµè§ˆå™¨å…¼å®¹æ€§)'),
                    ('h264', 'H.264'),
                    ('mp4v', 'MPEG-4'),
                ]
                
                outputvideo = None
                used_codec = None
                
                for codec, desc in codecs_to_try:
                    try:
                        fourcc = cv2.VideoWriter_fourcc(*codec)
                        test_writer = cv2.VideoWriter(output_mp4, fourcc, fps, (width, height))
                        
                        if test_writer.isOpened():
                            outputvideo = test_writer
                            used_codec = f"{codec} ({desc})"
                            config.output = output_mp4
                            print(f"âœ“ ä½¿ç”¨ {used_codec} ç¼–è§£ç å™¨è¾“å‡º: {output_mp4}")
                            break
                        else:
                            test_writer.release()
                    except Exception as e:
                        print(f"âš  {codec} ç¼–è§£ç å™¨å¤±è´¥: {e}")
                        continue
                
                # å¦‚æœæ‰€æœ‰MP4ç¼–è§£ç å™¨éƒ½å¤±è´¥ï¼Œå›é€€åˆ°AVI
                if not outputvideo or not outputvideo.isOpened():
                    print("âš  æ‰€æœ‰MP4ç¼–è§£ç å™¨å¤±è´¥ï¼Œå›é€€åˆ°AVIæ ¼å¼")
                    fourcc = cv2.VideoWriter_fourcc(*'XVID')
                    outputvideo = cv2.VideoWriter(config.output, fourcc, fps, (width, height))
                    if outputvideo.isOpened():
                        used_codec = "XVID (AVI)"
                        print(f"âœ“ ä½¿ç”¨ {used_codec} ç¼–è§£ç å™¨")
                    else:
                        print("âŒ æ‰€æœ‰è§†é¢‘ç¼–è§£ç å™¨éƒ½å¤±è´¥")
                        outputvideo = None
            
            while not cap.end:
                ret, img = cap.read()
                if not ret:
                    continue
                
                processed_frames += 1
                
                # YOLOæ£€æµ‹
                yolo_results = self.yolo_model.predict(
                    source=img, 
                    imgsz=config.imsize, 
                    device=config.device, 
                    verbose=False
                )
                boxes = yolo_results[0].boxes
                
                # å¤„ç†æ£€æµ‹ç»“æœ
                if len(boxes) > 0:
                    pred_xyxy = boxes.xyxy.cpu().numpy()
                    pred_conf = boxes.conf.cpu().numpy().reshape(-1, 1)
                    pred_cls = boxes.cls.cpu().numpy().reshape(-1, 1)
                    
                    pred = np.hstack((pred_xyxy, pred_conf, pred_cls))
                    xywh = np.hstack(((pred[:, 0:2] + pred[:, 2:4]) / 2, pred[:, 2:4] - pred[:, 0:2]))
                    
                    # DeepSortè·Ÿè¸ª
                    temp = deepsort_update(self.deepsort_tracker, pred, xywh, img)
                    temp = temp if len(temp) else np.ones((0, 8)).astype(np.float32)
                    
                                    # è¡Œä¸ºè¯†åˆ«ï¼ˆSlowFastï¼‰ - å…ˆè¿›è¡Œè¡Œä¸ºè¯†åˆ«å†ç»˜åˆ¶è§†é¢‘å¸§
                if len(cap.stack) == 25:
                    clip = cap.get_video_clip()
                    if temp.shape[0] > 0:
                        try:
                            # è·å–è¾¹ç•Œæ¡†ä½ç½®ï¼ˆå‰4åˆ—ï¼‰
                            boxes = temp[:, 0:4].astype(np.float32)
                            track_ids = temp[:, 5].astype(np.int32)  # è·Ÿè¸ªIDåœ¨ç¬¬5åˆ—
                            
                            inputs, inp_boxes, _ = ava_inference_transform(clip, boxes, crop_size=config.imsize)
                            
                            # ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜
                            inp_boxes = inp_boxes.float()  # ç¡®ä¿ä¸ºfloatç±»å‹
                            inp_boxes = torch.cat([torch.zeros(inp_boxes.shape[0], 1), inp_boxes], dim=1)
                            
                            if isinstance(inputs, list):
                                inputs = [inp.unsqueeze(0).to(self.device) for inp in inputs]
                            else:
                                inputs = inputs.unsqueeze(0).to(self.device)
                            
                            with torch.no_grad():
                                slowfaster_preds = self.video_model(inputs, inp_boxes.to(self.device))
                            
                            # è·å–é¢„æµ‹ç»“æœ
                            pred_labels = torch.argmax(slowfaster_preds.cpu(), axis=1).numpy()
                            
                            # æ›´æ–°è¡Œä¸ºæ ‡ç­¾æ˜ å°„
                            for tid, avalabel in zip(track_ids, pred_labels):
                                if avalabel < len(self.ava_labelnames):
                                    id_to_ava_labels[int(tid)] = self.ava_labelnames[avalabel + 1]
                                    
                            print(f"âœ“ SlowFastæ£€æµ‹åˆ°{len(pred_labels)}ä¸ªè¡Œä¸ºï¼Œæ›´æ–°æ ‡ç­¾æ˜ å°„")
                            
                        except Exception as e:
                            print(f"SlowFastå¤„ç†é”™è¯¯: {e}")
                            import traceback
                            traceback.print_exc()

                # åˆ›å»ºå¯è§†åŒ–å›¾åƒ - åœ¨è¡Œä¸ºè¯†åˆ«å®Œæˆåç»˜åˆ¶
                vis_img = img.copy()
                cv2.putText(vis_img, f'Frame: {processed_frames}', 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # å­˜å‚¨æ£€æµ‹ç»“æœå¹¶ç»˜åˆ¶ï¼ˆåŒ…å«æœ€æ–°çš„è¡Œä¸ºä¿¡æ¯ï¼‰
                for detection in temp:
                    if len(detection) >= 7:
                        x1, y1, x2, y2 = int(detection[0]), int(detection[1]), int(detection[2]), int(detection[3])
                        class_id = int(detection[4])  # è¿™æ˜¯YOLOçš„ç±»åˆ«ID
                        track_id = int(detection[5])  # è¿™æ˜¯DeepSortçš„è·Ÿè¸ªID
                        confidence = float(detection[6])
                        
                        # æ˜ å°„YOLOç±»åˆ«IDåˆ°ç±»åˆ«åç§°
                        object_type = 'unknown'
                        if 0 <= class_id < len(self.coco_names):
                            object_type = self.coco_names[class_id]
                        
                        # è·å–æœ€æ–°çš„è¡Œä¸ºæ ‡ç­¾
                        behavior_type = id_to_ava_labels.get(track_id, 'walking')
                        
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        color = (0, 0, 255) if self._is_anomaly_behavior(behavior_type) else (0, 255, 0)
                        cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, 2)
                        
                        # ç»˜åˆ¶å¯¹è±¡æ ‡ç­¾ï¼ˆåŒ…å«è¡Œä¸ºä¿¡æ¯ï¼‰
                        label1 = f"ID:{track_id} {object_type}"
                        label2 = f"Action: {behavior_type}"  # ä½¿ç”¨è‹±æ–‡é¿å…ä¸­æ–‡ä¹±ç 
                        
                        cv2.putText(vis_img, label1, (x1, y1-25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                        cv2.putText(vis_img, label2, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                        
                        result = {
                            'frame_number': processed_frames,
                            'timestamp': processed_frames / 25.0,
                            'object_id': track_id,
                            'object_type': object_type,
                            'confidence': confidence,
                            'bbox': {
                                'x1': float(detection[0]),
                                'y1': float(detection[1]),
                                'x2': float(detection[2]),
                                'y2': float(detection[3])
                            },
                            'behavior_type': behavior_type,
                            'is_anomaly': self._is_anomaly_behavior(behavior_type)
                        }
                        results.append(result)
                
                # å†™å…¥è§†é¢‘å¸§ï¼ˆä¿®å¤å¸§æ ¼å¼é—®é¢˜ï¼‰
                if outputvideo and outputvideo.isOpened():
                    # ç¡®ä¿å¸§å°ºå¯¸æ­£ç¡®
                    if vis_img.shape[:2] != (height, width):
                        vis_img = cv2.resize(vis_img, (width, height))
                    
                    # ç¡®ä¿å¸§æ ¼å¼æ­£ç¡®ï¼ˆBGRï¼‰
                    if len(vis_img.shape) == 3 and vis_img.shape[2] == 3:
                        success = outputvideo.write(vis_img)
                        if not success:
                            print(f"âš  å†™å…¥è§†é¢‘å¸§å¤±è´¥: å¸§ {processed_frames}, å°ºå¯¸: {vis_img.shape}")
                    else:
                        print(f"âš  å¸§æ ¼å¼é”™è¯¯: {vis_img.shape}")
                        # è½¬æ¢ä¸ºBGRæ ¼å¼
                        if len(vis_img.shape) == 2:
                            vis_img = cv2.cvtColor(vis_img, cv2.COLOR_GRAY2BGR)
                        success = outputvideo.write(vis_img)
                
                # æ›´æ–°è¿›åº¦
                if progress_callback and total_frames > 0:
                    progress = (processed_frames / total_frames) * 100
                    progress_callback(task_id, progress)
                
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                with self.task_lock:
                    if task_id in self.current_tasks and self.current_tasks[task_id]['status'] == 'stopped':
                        break
            
            # æ¸…ç†èµ„æº
            cap.release()
            if outputvideo:
                outputvideo.release()
            
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
                if os.path.exists(config.output):
                    file_size = os.path.getsize(config.output)
                    print(f"âœ“ è§†é¢‘ä¿å­˜æˆåŠŸ: {config.output} ({file_size} bytes)")
                else:
                    print(f"âŒ è¾“å‡ºæ–‡ä»¶æœªç”Ÿæˆ: {config.output}")
            
        except Exception as e:
            print(f"æ£€æµ‹è¿‡ç¨‹é”™è¯¯: {e}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            # ç¡®ä¿èµ„æºæ¸…ç†
            try:
                if 'cap' in locals() and cap:
                    cap.release()
            except:
                pass
            
            try:
                if 'outputvideo' in locals() and outputvideo:
                    outputvideo.release()
            except:
                pass
            
            # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯é‡æ–°æŠ›å‡ºå¼‚å¸¸
            return []
        
        return results
    
    def _run_realtime_detection(self, config, task_id: str, websocket_callback: callable = None):
        """
        æ‰§è¡Œå®æ—¶æ£€æµ‹çš„æ ¸å¿ƒé€»è¾‘
        """
        try:
            cap = MyVideoCapture(config.input)
            id_to_ava_labels = {}
            frame_count = 0
            
            while not cap.end:
                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                with self.task_lock:
                    if task_id in self.current_tasks and self.current_tasks[task_id]['status'] != 'running':
                        break
                
                ret, img = cap.read()
                if not ret:
                    continue
                
                frame_count += 1
                
                # YOLOæ£€æµ‹
                yolo_results = self.yolo_model.predict(
                    source=img, 
                    imgsz=config.imsize, 
                    device=config.device, 
                    verbose=False
                )
                boxes = yolo_results[0].boxes
                
                # å¤„ç†æ£€æµ‹ç»“æœ
                detections = []
                if len(boxes) > 0:
                    pred_xyxy = boxes.xyxy.cpu().numpy()
                    pred_conf = boxes.conf.cpu().numpy().reshape(-1, 1)
                    pred_cls = boxes.cls.cpu().numpy().reshape(-1, 1)
                    
                    pred = np.hstack((pred_xyxy, pred_conf, pred_cls))
                    xywh = np.hstack(((pred[:, 0:2] + pred[:, 2:4]) / 2, pred[:, 2:4] - pred[:, 0:2]))
                    
                    # DeepSortè·Ÿè¸ª
                    temp = deepsort_update(self.deepsort_tracker, pred, xywh, img)
                    temp = temp if len(temp) else np.ones((0, 8)).astype(np.float32)
                    
                    # æ ¼å¼åŒ–æ£€æµ‹ç»“æœ
                    for detection in temp:
                        if len(detection) >= 7:
                            detection_data = {
                                'frame_number': frame_count,
                                'timestamp': time.time(),
                                'object_id': int(detection[4]),
                                'object_type': 'person',
                                'confidence': float(detection[6]),
                                'bbox': {
                                    'x1': float(detection[0]),
                                    'y1': float(detection[1]),
                                    'x2': float(detection[2]),
                                    'y2': float(detection[3])
                                },
                                'behavior_type': id_to_ava_labels.get(int(detection[4]), 'unknown'),
                                'is_anomaly': False
                            }
                            detections.append(detection_data)
                    
                    # è¡Œä¸ºè¯†åˆ«ï¼ˆSlowFastï¼‰
                    if len(cap.stack) == 25:
                        clip = cap.get_video_clip()
                        if temp.shape[0] > 0:
                            try:
                                inputs, inp_boxes, _ = ava_inference_transform(clip, temp[:, 0:4], crop_size=config.imsize)
                                inp_boxes = torch.cat([torch.zeros(inp_boxes.shape[0], 1), inp_boxes], dim=1)
                                
                                if isinstance(inputs, list):
                                    inputs = [inp.unsqueeze(0).to(config.device) for inp in inputs]
                                else:
                                    inputs = inputs.unsqueeze(0).to(config.device)
                                
                                with torch.no_grad():
                                    slowfaster_preds = self.video_model(inputs, inp_boxes.to(config.device))
                                
                                for tid, avalabel in zip(temp[:, 5].tolist(), np.argmax(slowfaster_preds.cpu(), axis=1).tolist()):
                                    behavior = self.ava_labelnames[avalabel + 1]
                                    id_to_ava_labels[tid] = behavior
                                    
                                    # æ›´æ–°æ£€æµ‹ç»“æœä¸­çš„è¡Œä¸ºä¿¡æ¯
                                    for det in detections:
                                        if det['object_id'] == tid:
                                            det['behavior_type'] = behavior
                                            det['is_anomaly'] = self._is_anomaly_behavior(behavior)
                            except Exception as e:
                                print(f"å®æ—¶SlowFastå¤„ç†é”™è¯¯: {e}")
                
                # å‘é€å®æ—¶ç»“æœ
                if websocket_callback and detections:
                    websocket_callback({
                        'type': 'detection_result',
                        'task_id': task_id,
                        'frame_number': frame_count,
                        'timestamp': time.time(),
                        'detections': detections
                    })
                
                # æ£€æŸ¥å¼‚å¸¸è¡Œä¸ºå¹¶å‘é€æŠ¥è­¦
                for detection in detections:
                    if detection['is_anomaly'] and websocket_callback:
                        websocket_callback({
                            'type': 'alert',
                            'task_id': task_id,
                            'alert_type': detection['behavior_type'],
                            'detection': detection
                        })
                
                # æ§åˆ¶å¸§ç‡
                time.sleep(0.04)  # çº¦25 FPS
            
            # æ¸…ç†èµ„æº
            cap.release()
            
        except Exception as e:
            print(f"å®æ—¶æ£€æµ‹é”™è¯¯: {e}")
            raise e
    
    def _is_anomaly_behavior(self, behavior: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå¼‚å¸¸è¡Œä¸º
        
        Args:
            behavior: è¡Œä¸ºåç§°
            
        Returns:
            bool: æ˜¯å¦ä¸ºå¼‚å¸¸è¡Œä¸º
        """
        if not behavior:
            return False
        
        behavior_lower = behavior.lower()
        return any(alert_behavior.lower() in behavior_lower for alert_behavior in self.alert_behaviors)
    
    def get_supported_behaviors(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„è¡Œä¸ºç±»å‹åˆ—è¡¨
        
        Returns:
            List[str]: è¡Œä¸ºç±»å‹åˆ—è¡¨
        """
        if self.ava_labelnames:
            return list(self.ava_labelnames.values())
        return []
    
    def update_config(self, new_config: Dict[str, Any]):
        """
        æ›´æ–°é…ç½®å‚æ•°
        
        Args:
            new_config: æ–°çš„é…ç½®å‚æ•°
        """
        self.config.update(new_config)
        
        # æ›´æ–°ç›¸å…³å‚æ•°
        if 'device' in new_config:
            self.device = new_config['device']
        if 'input_size' in new_config:
            self.input_size = new_config['input_size']
        if 'confidence_threshold' in new_config:
            self.confidence_threshold = new_config['confidence_threshold']
        if 'alert_behaviors' in new_config:
            self.alert_behaviors = new_config['alert_behaviors']


# å…¨å±€æ£€æµ‹æœåŠ¡å®ä¾‹
detection_service = None

def get_detection_service(config: Dict[str, Any] = None) -> BehaviorDetectionService:
    """
    è·å–æ£€æµ‹æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        config: é…ç½®å‚æ•°
        
    Returns:
        BehaviorDetectionService: æ£€æµ‹æœåŠ¡å®ä¾‹
    """
    global detection_service
    if detection_service is None:
        detection_service = BehaviorDetectionService(config)
    return detection_service 